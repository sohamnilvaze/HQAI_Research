{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.datasets import mnist\nimport numpy as np\nimport torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom skimage.transform import resize\nfrom pathlib import Path\nfrom typing import Tuple\n\nBATCH_SIZE  = 500\nEPOCHS      = 10          \nLR          = 1e-1\nDEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Load MNIST using Keras\nprint(\"Loading the MNIST dataset\")\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\ndef resize_and_normalize(vec28: np.ndarray) -> np.ndarray:\n    \"\"\"Down-sample 28×28 → 8×8, flatten to length-64, ℓ2-normalize.\"\"\"\n    img = vec28.reshape(28, 28)\n    img8 = resize(img, (8, 8), anti_aliasing=True)\n    flat = img8.flatten()\n    norm = np.linalg.norm(flat) + 1e-10\n    return flat / norm\n  \n\ndef to_density(vec64: np.ndarray) -> np.ndarray:\n    \"\"\"|ψ⟩⟨ψ| to get 64×64 Hermitian rank-1 density matrix.\"\"\"\n    return np.outer(vec64, vec64.conjugate())\n\n\ndef build_dataset(images,labels):\n    vectors = np.stack([resize_and_normalize(im) for im in images])\n    density = np.stack([to_density(v) for v in vectors])\n    return density, labels\n\nclass HermitianDataset(Dataset):\n    def __init__(self, mats: np.ndarray, labels: np.ndarray):\n        self.mats = torch.tensor(mats, dtype=torch.complex64)\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self): return len(self.mats)\n    def __getitem__(self, idx):\n        return self.mats[idx], self.labels[idx]\n\n\nclass MatrixModel(nn.Module):\n    def __init__(self, num_classes=10, dim=64):\n        super().__init__()\n        self.dim = dim\n        self.num_classes = num_classes\n        self.gen_real = nn.Parameter(torch.randn(num_classes, dim, dim))\n        self.gen_imag = nn.Parameter(torch.randn(num_classes, dim, dim))\n        self.eigvals   = nn.Parameter(torch.randn(num_classes, dim))\n\n    def _unitary(self, gen_r, gen_i):\n        g = torch.complex(gen_r, gen_i)\n        herm = (g + g.conj().transpose(-2, -1)) / 2\n        return torch.matrix_exp(-1j * herm)\n\n    def hamiltonians(self):\n        U = self._unitary(self.gen_real, self.gen_imag)           \n        D = torch.diag_embed(torch.complex(self.eigvals, torch.zeros_like(self.eigvals)))\n        H = U @ D @ U.conj().transpose(-2, -1)                 \n        return (H + H.conj().transpose(-2, -1)) / 2                \n\n    def forward(self):                                         \n        return self.hamiltonians()\n\n\ndef prototype_loss(protos, batch_mats, labels):\n    B = batch_mats.size(0)\n    target = protos[labels]                                \n    return torch.linalg.norm(batch_mats - target, dim=(1, 2)).mean()\n\ndef train_model(model, loader):\n    opt = optim.Adam(model.parameters(), lr=LR)\n    lr_sched = optim.lr_scheduler.ReduceLROnPlateau(opt, patience=3, factor=0.5)\n\n    for epoch in range(EPOCHS):\n        print(f\"Epoch: {epoch}\")\n        model.train()\n        running = 0\n        for mats, lbl in loader:\n            mats, lbl = mats.to(DEVICE), lbl.to(DEVICE)\n            opt.zero_grad()\n            loss = prototype_loss(model.hamiltonians(), mats, lbl)\n            loss.backward()\n            opt.step()\n            running += loss.item()\n        avg = running / len(loader)\n        lr_sched.step(avg)\n        print(f\"Epoch {epoch+1:02d}/{EPOCHS}  •  loss {avg:.6f}\")\n\n    return model.hamiltonians().detach().cpu().numpy()\n\n\nprint(\"Preparing data\")\ntrain_mats, train_labels = build_dataset(x_train, y_train)\n\nprint(f\"Class distribution: {np.bincount(train_labels)}\")\nds = HermitianDataset(train_mats,train_labels)\ndl = DataLoader(ds,batch_size = BATCH_SIZE, shuffle = True)\n\nprint(\"Training...\")\nmodel = MatrixModel().to(DEVICE)\nhams = train_model(model,dl)\n\nfor k in range(10):\n    herm = np.allclose(hams[k], hams[k].conj().T)\n    frob = np.linalg.norm(hams[k])\n    print(f\"Class {k}: Hermitian={herm}, Frobenius norm={frob:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T11:34:48.404633Z","iopub.execute_input":"2025-08-16T11:34:48.404979Z","iopub.status.idle":"2025-08-16T11:37:45.643028Z","shell.execute_reply.started":"2025-08-16T11:34:48.404953Z","shell.execute_reply":"2025-08-16T11:37:45.641668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ML Gradient Pipeline Finding the Quantum Harmonic Oscillator Hamiltonian Truncated to 64 levels for MNIST Dataset...\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\nclass SharedHamiltonian(nn.Module):\n    def __init__(self, N, m=1.0, omega=1.0, hbar=1.0):\n        super().__init__()\n        self.xcap = nn.Parameter(torch.randn(N, N, dtype=torch.complex128) * 1e-2)\n        self.pcap = nn.Parameter(torch.randn(N, N, dtype=torch.complex128) * 1e-2)\n        self.m = torch.tensor(m, dtype=torch.complex128)\n        self.omega = torch.tensor(omega, dtype=torch.complex128)\n        self.hbar = torch.tensor(hbar, dtype=torch.complex128)\n\n    def forward(self):\n        x = (self.xcap + self.xcap.conj().T) * 0.5\n        p = (self.pcap + self.pcap.conj().T) * 0.5\n        H = (p @ p.conj().T) / (2 * self.m) + 0.5 * self.m * (self.omega**2) * (x @ x.conj().T)\n        return H\n\n    def commutator_constraint(self):\n        x = (self.xcap + self.xcap.conj().T) * 0.5\n        p = (self.pcap + self.pcap.conj().T) * 0.5\n        I = torch.eye(x.shape[0], dtype=torch.complex128, device=x.device)\n        comm = x @ p - p @ x\n        target = 1j * self.hbar * I\n        return torch.mean(torch.abs(comm - target)**2)\n\n\ndef train_shared_parameters(hamiltonians, num_epochs_stage1=500, num_epochs_stage2=500,\n                            lr=1e-4, comm_weight_stage1=1e5, comm_weight_stage2=1e4,\n                            recon_weight=1.0, spectral_alpha=1.0, device=None):\n    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    targets = [torch.tensor(h, dtype=torch.complex128, device=device) for h in hamiltonians]\n    N =  64\n\n    model = SharedHamiltonian(N).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n\n    def step(epoch, comm_w, recon_w):\n        optimizer.zero_grad()\n        H_pred = model()\n        recon_real = sum(F.mse_loss(H_pred.real, H_t.real) for H_t in targets)\n        recon_imag = sum(F.mse_loss(H_pred.imag, H_t.imag) for H_t in targets)\n        recon_loss = recon_real + recon_imag\n        comm_loss = model.commutator_constraint()\n        loss = comm_w * comm_loss + recon_w * recon_loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        if epoch % 50 == 0 or epoch == 1:\n            print(f\"Epoch {epoch:4d} | CommLoss={comm_loss.item():.2e} | ReconLoss={recon_loss.item():.2e}\")\n\n    # Stage 1: commutator Loss redcution\n    for epoch in range(1, num_epochs_stage1 + 1):\n        step(epoch, comm_weight_stage1, 0.0)\n\n    # Stage 2: Fine-tune reconstruction of the parameters obtained from stage 1\n    for epoch in range(1, num_epochs_stage2 + 1):\n        step(epoch, comm_weight_stage2, recon_weight)\n\n    xcap_np = model.xcap.detach().cpu().numpy()\n    pcap_np = model.pcap.detach().cpu().numpy()\n    H_shared_np = model().detach().cpu().numpy()\n    mass_np = model.m.detach().cpu().numpy()\n    hbar_np = model.hbar.detach().cpu().numpy()\n    omega_np = model.omega.detach().cpu().numpy()\n    return xcap_np, pcap_np, H_shared_np , mass_np , hbar_np , omega_np\n\ntargets = hams\n\nxcap, pcap, H_shared , mass_np , hbar_np , omega_np = train_shared_parameters(\n        targets,\n        num_epochs_stage1=100,\n        num_epochs_stage2=100,\n        lr=1e-4,\n        comm_weight_stage1=1,\n        comm_weight_stage2=1,\n        recon_weight=1.0,\n        spectral_alpha=1.0\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T11:40:16.035450Z","iopub.execute_input":"2025-08-16T11:40:16.035919Z","iopub.status.idle":"2025-08-16T11:40:17.837905Z","shell.execute_reply.started":"2025-08-16T11:40:16.035890Z","shell.execute_reply":"2025-08-16T11:40:17.834933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Here xcap and pcap parameter are truncated upto 64 levels and hence they do not satisfy the commutative constraint in finite level case\n# Commutative Relation in Finite Level Case not equal to i*I\nx = (xcap + xcap.conj().T) * 0.5\np = (pcap + pcap.conj().T) * 0.5\ncommutator = x @ p - p @ x\nprint(np.trace(commutator)) # not equal to 64*i # Mathematical Limit for truncation...\nprint(hams.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T11:40:48.568731Z","iopub.execute_input":"2025-08-16T11:40:48.569150Z","iopub.status.idle":"2025-08-16T11:40:48.578085Z","shell.execute_reply.started":"2025-08-16T11:40:48.569121Z","shell.execute_reply":"2025-08-16T11:40:48.576936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Imposing Hermitian Constraints on the Position and the Momentum Operators if not already satisfied -\nm , omega = mass_np , omega_np\nH_QHO_MNIST = (p @ p.conj().T) / (2 * m) + 0.5 * m * (omega**2) * (x @ x.conj().T)\n\n# Plot of Quantum Harmonic Oscillator for MNIST for 6 levels i.e. 64 x 64 Hamiltonian Matrix ...\n\n# Energy Eigen Values and Energy Eigen States (Eigen Vectors)\n\neig_fn = np.linalg.eigh\n\nenergyeigenvalues, energyeigenvectors = eig_fn(H_QHO_MNIST)\nEnergy_Eigen_States = energyeigenvectors\nEnergy_Eigen_States_conj_T = energyeigenvectors.T.conj() \nEnergy_Eigen_Values = energyeigenvalues\n\n# Position Eigen Values and Position Eigen States (Eigen Vectors)\n\npositioneigenvalues, positioneigenvectors = eig_fn(x)\nPosition_Eigen_States = positioneigenvectors\nPosition_Eigen_States_conj_T = positioneigenvectors.T.conj() \nPosition_Eigen_Values = positioneigenvalues","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T11:41:19.572478Z","iopub.execute_input":"2025-08-16T11:41:19.572831Z","iopub.status.idle":"2025-08-16T11:41:19.636571Z","shell.execute_reply.started":"2025-08-16T11:41:19.572806Z","shell.execute_reply":"2025-08-16T11:41:19.635200Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Plot for Non-Normalized case","metadata":{}},{"cell_type":"code","source":"#Visualizing quantum states by plotting probability amplitudes of eigenstates of a hamiltonian against position x and energy E.\n#After diagonalising the Hamiltonian H, we get energy levels as eigenvalues and wavefunctions in energy basis as eigenvectors\n#\n\n\n# Non - Normalized Visualization of the Probability Amplitudes...\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import UnivariateSpline\nfrom matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec\nimport os\n\ndef plot_quantum_states_comprehensive(E_vals, E_vecs, X_vals, X_vecs):\n    nE, nX = len(E_vals), len(X_vals)\n    # Non-Normalized Probabilities per energy Level\n    amps = np.abs(E_vecs.conj().T @ X_vecs) ** 2\n    # Normalized Probability Amplitudes per energy level\n    # Uncomment for Normalized Case\n    # amps_raw = np.abs(E_vecs.conj().T @ X_vecs) ** 2  # shape (nE, nX)\n\n    # # Normalize each energy row by its RMS (root sum of squares)\n    # rms = np.sqrt(np.sum(amps_raw**2, axis=1, keepdims=True))  # shape (nE, 1)\n    # amps = amps_raw / rms\n\n    cmap_pieces = [\n        plt.cm.tab20(np.linspace(0, 1, 20)),\n        plt.cm.Set3(np.linspace(0, 1, 12)),\n        plt.cm.Dark2(np.linspace(0, 1, 8)),\n        plt.cm.Accent(np.linspace(0, 1, 8)),\n        plt.cm.Paired(np.linspace(0, 1, 12)),\n        plt.cm.Set1(np.linspace(0, 1, 4)),\n    ]\n    colors = np.vstack(cmap_pieces)[:nE]\n\n    def envelope(xv, yv, a):\n        pts = [\n            (xv[i], yv[np.argmax(a[:, i])])\n            for i in range(len(xv))\n            if a[:, i].max() > 1e-3\n        ]\n        if len(pts) < 5:\n            return None, None\n        pts.sort(key=lambda p: p[0])\n        xs, ys = zip(*pts)\n        spline = UnivariateSpline(xs, ys, s=len(xs) * 0.1)\n        xsmooth = np.linspace(xs[0], xs[-1], 300)\n        return xsmooth, spline(xsmooth)\n\n    def plot_panel(ax, zx, zy, title, version, legend_full=False):\n        for E in E_vals:\n            ax.axhline(E, color='lightgray', lw=0.5, alpha=0.3)\n        if version == 1:\n            for k, E in enumerate(E_vals):\n                c = amps[k]\n                ax.scatter(X_vals, np.full(nX, E), c=c, cmap='plasma',\n                           s=c * 1200, edgecolors='black', lw=0.4, alpha=0.7)\n            cb = plt.colorbar(ax.collections[-1], ax=ax, shrink=0.8)\n            cb.set_label('|⟨x|ψ⟩|²')\n        else:\n            handles = []\n            for k, E in enumerate(E_vals):\n                sz = amps[k] * 700\n                ax.scatter(X_vals, np.full(nX, E), color=colors[k], s=sz,\n                           edgecolors='black', lw=0.3, alpha=0.8)\n                if legend_full:\n                    handles.append(plt.Line2D([0], [0], marker='o', color='w',\n                                              markerfacecolor=colors[k], markersize=6,\n                                              label=f'ψ_{k}: E={E:.3f}'))\n        xv, yv = envelope(X_vals, E_vals, amps)\n        if xv is not None:\n            ax.plot(xv, yv, 'r--', lw=2, label='V(x) envelope')\n            if legend_full:\n                handles.append(plt.Line2D([0], [0], color='red', linestyle='--',\n                                          lw=2, label='V(x) envelope'))\n        if zx:\n            ax.set_xlim(*zx)\n        if zy:\n            ax.set_ylim(*zy)\n        ax.set_xlabel('x')\n        ax.set_ylabel('E')\n        ax.set_title(title, fontweight='bold')\n        ax.grid(alpha=0.3)\n        if legend_full:\n            ax.legend(handles=handles, bbox_to_anchor=(1.02, 1),\n                      loc='upper left', fontsize=7, ncol=4,\n                      title='All 64 Energy Levels')\n        \n        plt.savefig(title, dpi=300, bbox_inches=\"tight\")\n        plt.close()\n        print(f\"Saved: {title}\")\n    Emin, Emax = E_vals.min(), E_vals.max()\n    xmin, xmax = X_vals.min(), X_vals.max()\n    dE, dX = Emax - Emin, xmax - xmin\n\n    views = [\n        (None, None, \"Full Range – Plasma Colormap\", 1, False),\n        (None, None, \"Full Range – 64-Level Colors + Legend\", 2, True),\n        None,  # blank slot\n        (None, (Emin, Emin + 0.20 * dE), \"Lowest 20% Energies\", 2, False),\n        (None, (Emin, Emin + 0.10 * dE), \"Lowest 10% Energies\", 2, False),\n        ((-0.2, 0.2), (Emin, Emin + 0.05 * dE), \"Lowest 5% Energies\", 2, False),\n        ((-0.2, 0.2), (Emin, Emin + 0.02 * dE), \"Lowest 2% Energies\", 2, False),\n        ((-0.125, 0.125), (Emin, Emin + 0.01 * dE), \"Lowest 1% Energies\", 2, False),\n        ((xmin + 0.45 * dX, xmax - 0.45 * dX), None, \"x≈0 Central 10%\", 2, False)\n    ]\n\n    rows = (len(views) + 2) // 3\n    fig = plt.figure(figsize=(24, 6 * rows))\n    outer = GridSpec(rows, 1, height_ratios=[1] * rows, hspace=0.3)\n\n    for r in range(rows):\n        if r == 0:\n            inner = GridSpecFromSubplotSpec(1, 3, subplot_spec=outer[r],\n                                            width_ratios=[1, 0.6, 1], wspace=0.12)\n        else:\n            inner = GridSpecFromSubplotSpec(1, 3, subplot_spec=outer[r],\n                                            width_ratios=[1, 1, 1], wspace=0.15)\n        for c in range(3):\n            idx = r * 3 + c\n            if idx >= len(views):\n                break\n            slot = views[idx]\n            ax = fig.add_subplot(inner[0, c])\n            if slot is not None:\n                zx, zy, title, ver, leg = slot\n                plot_panel(ax, zx, zy, title, ver, leg)\n            else:\n                ax.axis('off')\n\n    fig.suptitle('Quantum States Probabilities E vs x: |⟨x|ψ⟩|² with V(x) Envelope (64 Levels)',\n                 fontsize=18, fontweight='bold', y=0.94)\n    plt.show()\n    return fig\n\n\ndef main():\n    eig = np.linalg.eigh\n    E_vals, E_vecs = eig(H_QHO_MNIST)\n    X_vals, X_vecs = eig(x)\n    plot_quantum_states_comprehensive(E_vals, E_vecs, X_vals, X_vecs)\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T11:46:42.692285Z","iopub.execute_input":"2025-08-16T11:46:42.692742Z","iopub.status.idle":"2025-08-16T11:46:46.805757Z","shell.execute_reply.started":"2025-08-16T11:46:42.692711Z","shell.execute_reply":"2025-08-16T11:46:46.803941Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Plots for Normalised case","metadata":{}},{"cell_type":"code","source":"# Normalized Visualization of the Probability Amplitudes...\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import UnivariateSpline\nfrom matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec\n\ndef plot_quantum_states_comprehensive(E_vals, E_vecs, X_vals, X_vecs):\n    nE, nX = len(E_vals), len(X_vals)\n    amps_raw = np.abs(E_vecs.conj().T @ X_vecs) ** 2  \n    rms = np.sqrt(np.sum(amps_raw**2, axis=1, keepdims=True)) \n    amps = amps_raw / rms\n    cmap_pieces = [\n        plt.cm.tab20(np.linspace(0, 1, 20)),\n        plt.cm.Set3(np.linspace(0, 1, 12)),\n        plt.cm.Dark2(np.linspace(0, 1, 8)),\n        plt.cm.Accent(np.linspace(0, 1, 8)),\n        plt.cm.Paired(np.linspace(0, 1, 12)),\n        plt.cm.Set1(np.linspace(0, 1, 4)),\n    ]\n    colors = np.vstack(cmap_pieces)[:nE]\n\n    def envelope(xv, yv, a):\n        pts = [\n            (xv[i], yv[np.argmax(a[:, i])])\n            for i in range(len(xv))\n            if a[:, i].max() > 1e-3\n        ]\n        if len(pts) < 5:\n            return None, None\n        pts.sort(key=lambda p: p[0])\n        xs, ys = zip(*pts)\n        spline = UnivariateSpline(xs, ys, s=len(xs) * 0.1)\n        xsmooth = np.linspace(xs[0], xs[-1], 300)\n        return xsmooth, spline(xsmooth)\n\n    def plot_panel(ax, zx, zy, title, version, legend_full=False):\n        for E in E_vals:\n            ax.axhline(E, color='lightgray', lw=0.5, alpha=0.3)\n\n        # Scatter points\n        if version == 1:\n            # Plasma colormap by normalized amplitude\n            for k, E in enumerate(E_vals):\n                c = amps[k]\n                ax.scatter(X_vals, np.full(nX, E),\n                           c=c, cmap='plasma',\n                           s=c * 1200,\n                           edgecolors='black', lw=0.4, alpha=0.7)\n            cb = plt.colorbar(ax.collections[-1], ax=ax, shrink=0.8)\n            cb.set_label('|⟨x|ψ⟩|² (normalized)', fontsize=10)\n        else:\n            handles = []\n            for k, E in enumerate(E_vals):\n                sz = amps[k] * 700\n                ax.scatter(X_vals, np.full(nX, E),\n                           color=colors[k], s=sz,\n                           edgecolors='black', lw=0.3, alpha=0.8)\n                if legend_full:\n                    handles.append(\n                        plt.Line2D([0], [0], marker='o', color='w',\n                                   markerfacecolor=colors[k], markersize=6,\n                                   label=f'ψ_{k}: E={E:.3f}')\n                    )\n\n        # Envelope V(x)\n        xv, yv = envelope(X_vals, E_vals, amps)\n        if xv is not None:\n            ax.plot(xv, yv, 'r--', lw=2, label='V(x) envelope')\n            if legend_full:\n                handles.append(\n                    plt.Line2D([0], [0], color='red', linestyle='--',\n                               lw=2, label='V(x) envelope')\n                )\n\n        # Apply zoom limits\n        if zx:\n            ax.set_xlim(*zx)\n        if zy:\n            ax.set_ylim(*zy)\n\n        # Labels and title\n        ax.set_xlabel('x')\n        ax.set_ylabel('E')\n        ax.set_title(title, fontweight='bold')\n        ax.grid(alpha=0.3)\n\n        # Legend for center plot\n        if legend_full:\n            ax.legend(handles=handles, bbox_to_anchor=(1.02, 1),\n                      loc='upper left', fontsize=7, ncol=4,\n                      title='All 64 Energy Levels', title_fontsize=9)\n\n        plt.savefig(title + \"norm\", dpi=300, bbox_inches=\"tight\")\n        plt.close()\n        print(f\"Saved: {title}+norm\")\n        \n\n    # Compute ranges\n    Emin, Emax = E_vals.min(), E_vals.max()\n    xmin, xmax = X_vals.min(), X_vals.max()\n    dE, dX = Emax - Emin, xmax - xmin\n\n    # View configurations\n    views = [\n        (None, None, \"Full Range – Plasma Colormap\", 1, False),\n        (None, None, \"Full Range – 64-Level Colors + Legend\", 2, True),\n        None,  # blank slot\n        (None, (Emin, Emin + 0.20 * dE), \"Lowest 20% Energies\", 2, False),\n        (None, (Emin, Emin + 0.10 * dE), \"Lowest 10% Energies\", 2, False),\n        ((-0.2, 0.2), (Emin, Emin + 0.05 * dE), \"Lowest 5% Energies\", 2, False),\n        ((-0.2, 0.2), (Emin, Emin + 0.02 * dE), \"Lowest 2% Energies\", 2, False),\n        ((-0.1, 0.1), (Emin, Emin + 0.01 * dE), \"Lowest 1% Energies\", 2, False),\n        ((xmin + 0.45 * dX, xmax - 0.45 * dX), None, \"x≈0 Central 10%\", 2, False),\n    ]\n\n    rows = (len(views) + 2) // 3\n    fig = plt.figure(figsize=(24, 6 * rows))\n    outer = GridSpec(rows, 1, height_ratios=[1] * rows, hspace=0.3)\n\n    for r in range(rows):\n        if r == 0:\n            inner = GridSpecFromSubplotSpec(1, 3, subplot_spec=outer[r],\n                                            width_ratios=[1, 0.6, 1], wspace=0.12)\n        else:\n            inner = GridSpecFromSubplotSpec(1, 3, subplot_spec=outer[r],\n                                            width_ratios=[1, 1, 1], wspace=0.15)\n        for c in range(3):\n            idx = r * 3 + c\n            ax = fig.add_subplot(inner[0, c])\n            slot = views[idx] if idx < len(views) else None\n            if slot:\n                zx, zy, title, version, legend_full = slot\n                plot_panel(ax, zx, zy, title, version, legend_full)\n            else:\n                ax.axis('off')\n\n    fig.suptitle('Quantum States Probabilities E vs x: |⟨x|ψ⟩|² with V(x) Envelope (64 Levels)',\n                 fontsize=18, fontweight='bold', y=0.94)\n    plt.show()\n    return fig\n\ndef main():\n    eig = np.linalg.eigh\n    E_vals, E_vecs = eig(H_QHO_MNIST)\n    X_vals, X_vecs = eig(x)\n    plot_quantum_states_comprehensive(E_vals, E_vecs, X_vals, X_vecs)\n\nif __name__ == \"__main__\":\n    main()\n\n# The Univariate Spline Can be made better (red line V(x)) fit , by passing it through extremums of the x per energy level where the |⟨x|ψ⟩|² is non zero .\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T11:48:34.203907Z","iopub.execute_input":"2025-08-16T11:48:34.204345Z","iopub.status.idle":"2025-08-16T11:48:38.980786Z","shell.execute_reply.started":"2025-08-16T11:48:34.204315Z","shell.execute_reply":"2025-08-16T11:48:38.979021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}