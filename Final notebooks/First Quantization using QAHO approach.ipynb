{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM07p/xuj6wkdch7iJ4wPXG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["In this approach, we implement a quantum inspired classification method using the principles of Quantum Anharmonic Oscillator. Here we model the classical MNIST dataset images using the linear combination of terms in position and momentum operators to resemble the potentrial and kinetic energy of an anharmonic oscillator.\n","\n","In this approach we have implemented 2 cases and remaninig 2 cases are improvements done on the 1st two cases."],"metadata":{"id":"wP8YFbVJiCEI"}},{"cell_type":"markdown","source":["Data preparation:\n","\n","We use the MNIST dataset consisting of 60000 training images and 1000 tet images.The original size of the images is (784,). We reshape the images to size (28,28), normalize the images and then convert the images into hamiltonians using 4 methods\n","\n","1) H = (A + A.T) / 2\n","\n","2)H = AA.T\n","\n","3)H = outer product of flattened image vectors\n","\n","4) H = -i*log(V) where V is a unitary matrix.\n","\n","Then all the hamiltonians are seperated to 10 classes on the basis of the digits from 0 to 9."],"metadata":{"id":"h7HnSffUipWU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XK9_9dl8gpsa"},"outputs":[],"source":["# Data Preprocessing of the MNIST Dataset to produce the train and test normalized Hamiitonians...\n","# We can construct the hamiltonians from the four methods described in the paper...\n","import numpy as np\n","from skimage.transform import resize\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import torch\n","from sklearn.datasets import fetch_openml\n","import scipy\n","from tensorflow.keras.datasets import mnist\n","\n","# Load MNIST using Keras\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# # Reshape and convert to float64 for consistency\n","x_train = x_train.reshape(-1, 784).astype(np.float64)\n","x_test = x_test.reshape(-1, 784).astype(np.float64)\n","print(\"Train:\", x_train.shape, y_train.shape)\n","print(\"Test:\", x_test.shape, y_test.shape)\n","\n","# ----------------------------\n","# Helper functions\n","# ----------------------------\n","def separate_digits(images, labels):\n","    \"\"\"Group images by digit label.\"\"\"\n","    digit_image = {d: [] for d in range(10)}\n","    for img, lbl in zip(images, labels):\n","        digit_image[lbl].append(img)\n","    return digit_image\n","\n","def resize_images_batch(images, new_size=(8, 8), batch_size=500):\n","    \"\"\"Resize a batch of flattened 28x28 images to new_size.\"\"\"\n","    n = len(images)\n","    resized = []\n","    for i in range(0, n, batch_size):\n","        batch = images[i:i+batch_size]\n","        resized_batch = [resize(img.reshape(28,28), new_size).flatten() for img in batch]\n","        resized.extend(resized_batch)\n","    return np.array(resized)\n","\n","def normalize_batch(images):\n","    \"\"\"Normalize each image vector.\"\"\"\n","    norms = np.linalg.norm(images, axis=1, keepdims=True)\n","    return images / norms\n","\n","#Creating Hamiltonian using outer product method\n","def density_matrix_batch(images):\n","    \"\"\"Convert vectors to density matrices.\"\"\"\n","    return np.matmul(images[:,:,np.newaxis], images[:,np.newaxis,:])\n","\n","#Creating the Hamiltonian using H = A + A.T/2 method\n","def hamiltonian_symmetric_batch(images):\n","  N,D = images.shape\n","  H_list = []\n","  for i in range(N):\n","    a = images[i]\n","    A = np.outer(a,np.ones(D))\n","    H = (A + A.conj().T) / 2\n","    H_list.append(H)\n","\n","  return np.array(H_list)\n","\n","#Creating the Hamiltonian using H = A @ A.T method\n","def hamiltonian_product_batch(images):\n","  N , D = images.shape\n","  H_list = []\n","  for i in range(N):\n","    a = images[i]\n","    A = np.outer(a,np.ones(D))\n","    H = A @ A.T\n","    H_list.append(H)\n","  return np.array(H_list)\n","\n","import scipy.linalg\n","#Creating the Hamiltonian using H = -i * log(V) method\n","def hamiltonian_using_log(images):\n","    def _make_hermitian(M):\n","        return 0.5 * (M + M.conj().transpose(-2,-1))\n","\n","    def _make_unitary(M):\n","        H = _make_hermitian(M)\n","        return torch.matrix_exp(-1j*H)\n","\n","    N,D = images.shape\n","    hamiltonians = np.zeros((N,D,D),dtype = np.complex128)\n","    for i in range(N):\n","        image = images[i]\n","        mat = np.diag(image)\n","        mat_torch = torch.tensor(mat,dtype = torch.complex128)\n","        H = _make_unitary(mat_torch)\n","        hamiltonians[i] = H\n","    return hamiltonians\n","\n","\n","# ----------------------------\n","# Process training data\n","# ----------------------------\n","digit_images_dict = separate_digits(x_train, y_train)\n","resized_digit_images = {}\n","normalized_digit_images = {}\n","density_matrices = {}\n","\n","for digit, imgs in digit_images_dict.items():\n","    imgs = np.array(imgs)\n","    imgs_resized = resize_images_batch(imgs, new_size=(8,8), batch_size=500)\n","    imgs_normalized = normalize_batch(imgs_resized)\n","    print(f\"normalized_images shape:- {imgs_normalized.shape}\")\n","    density1 = density_matrix_batch(imgs_normalized)\n","    print(f\"shape 1:- {density1.shape}\")\n","    #OR\n","    #density2 = hamiltonian_symmetric_batch(imgs_normalized)\n","    #print(f\"shape 2:- {density2.shape}\")\n","    #OR\n","    #density3 = hamiltonian_product_batch(imgs_normalized)\n","    #print(f\"shape 3:- {density3.shape}\")\n","    #OR\n","    #density4 = hamiltonian_using_log(imgs_normalized)\n","    #print(f\"shape 4:- {density4.shape}\")\n","    density1 /= np.linalg.norm(density1, axis=(1,2), keepdims=True)\n","    #density2 /= np.linalg.norm(density2, axis=(1,2), keepdims=True)\n","    #density3 /= np.linalg.norm(density3, axis=(1,2), keepdims=True)\n","    #density4 /= np.linalg.norm(density4, axis=(1,2), keepdims=True)\n","    resized_digit_images[digit] = imgs_resized\n","    normalized_digit_images[digit] = imgs_normalized\n","    density_matrices[digit] = density1\n","    #density_matrices[digit] = density2\n","    #density_matrices[digit] = density3\n","    #density_matrices[digit] = density4\n","\n","train_density_matrices = np.concatenate([density_matrices[d] for d in range(10)], axis=0)\n","train_density_matrices_tensor = torch.tensor(train_density_matrices, dtype=torch.cfloat)\n","\n","# ----------------------------\n","# Process test data\n","# ----------------------------\n","test_images_resized = np.array([resize(img.reshape(28,28), (8,8)).flatten() for img in x_test])\n","test_normed = normalize_batch(test_images_resized)\n","test_density = density_matrix_batch(test_normed)\n","test_density /= np.linalg.norm(test_density, axis=(1,2), keepdims=True)\n","test_density_tensor = torch.tensor(test_density, dtype=torch.cfloat)\n","\n","# ----------------------------\n","# Visualization example\n","# ---------------------------\n","\n","for digit in range(10):\n","    images_to_plot = resized_digit_images[digit][:10]\n","    plt.figure(figsize=(10,2))\n","    for i in range(10):\n","        plt.subplot(1, 10, i+1)\n","        plt.imshow(images_to_plot[i].reshape(8,8), cmap='magma')\n","        plt.title(f\"{digit}\")\n","        plt.axis('off')\n","    plt.show()\n","\n","normalized_Hermitian_Digit_matrices = train_density_matrices_tensor\n","normalized_hermitian_matrices_test_input = test_density_tensor\n","\n","print(f\"normalized_Hermitian_Digit_matrices shape:- {normalized_Hermitian_Digit_matrices.shape}\")\n","print(f\"normalized_hermitian_matrices_test_input shape:- {normalized_hermitian_matrices_test_input.shape}\")\n","\n","labels = []\n","for i in range(10):\n","    labels.append(i)\n","\n","print(labels)\n","\n","D = [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n","# D = [100] * 10\n","labels_zero = [labels[0]]*D[0]\n","labels_one  = [labels[1]]*D[1]\n","labels_two  = [labels[2]]*D[2]\n","labels_three  = [labels[3]]*D[3]\n","labels_four  = [labels[4]]*D[4]\n","labels_five  = [labels[5]]*D[5]\n","labels_six  = [labels[6]]*D[6]\n","labels_seven  = [labels[7]]*D[7]\n","labels_eigth  = [labels[8]]*D[8]\n","labels_nineth  = [labels[9]]*D[9]\n","labels_zero = np.array(labels_zero,dtype = int)\n","labels_one = np.array(labels_one,dtype = int)\n","labels_two = np.array(labels_two,dtype = int)\n","labels_three = np.array(labels_three,dtype = int)\n","labels_four = np.array(labels_four,dtype = int)\n","labels_five = np.array(labels_five,dtype = int)\n","labels_six = np.array(labels_six,dtype = int)\n","labels_seven = np.array(labels_seven,dtype = int)\n","labels_eigth = np.array(labels_eigth,dtype = int)\n","labels_nineth = np.array(labels_nineth,dtype = int)\n","\n","labels_new_train = np.concatenate((labels_zero,labels_one))\n","labels_new_train = np.concatenate((labels_new_train,labels_two))\n","labels_new_train = np.concatenate((labels_new_train,labels_three))\n","labels_new_train = np.concatenate((labels_new_train,labels_four))\n","labels_new_train = np.concatenate((labels_new_train,labels_five))\n","labels_new_train = np.concatenate((labels_new_train,labels_six))\n","labels_new_train = np.concatenate((labels_new_train,labels_seven))\n","labels_new_train = np.concatenate((labels_new_train,labels_eigth))\n","labels_new_train = np.concatenate((labels_new_train,labels_nineth))\n"]},{"cell_type":"markdown","source":["**Case 1:-**\n","\n","In this we model the classical MNIST images using a linear combination of trainable base matrices. The model learns 2 things simaltaneously: image-specefic coefficients that define the combination ands a single unitary transformation matrix which is trained to rotate each reconstructed Hamiltonian into a pre-defined diagonal target matrix effectively mapping each image to unique class specific quantum state.Classification is then done by simply checking the position of largest diagonal element in transformed test hamiltonian."],"metadata":{"id":"84mn1_TKisMX"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import time\n","from typing import Tuple\n","\n","def set_seed(seed: int = 42) -> None:\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","\n","class IntegratedHermitianClassifier(nn.Module):\n","    def __init__(\n","        self,\n","        matrix_size: int = 64,\n","        d_order: int = 3,\n","        lr: float = 1e-3,\n","        epochs: int = 50,\n","        batch_size: int = 128,\n","        device: str = None,\n","    ) -> None:\n","        super().__init__()\n","\n","        # Hyper-parameters\n","        self.n = matrix_size\n","        self.d = d_order\n","        self.lr = lr\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.device = torch.device(device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n","\n","        # Trainable Hermitian bases (small init for stability)\n","        self.x = nn.Parameter(0.1 * torch.randn(matrix_size, matrix_size, dtype=torch.cfloat))\n","        self.p = nn.Parameter(0.1 * torch.randn(matrix_size, matrix_size, dtype=torch.cfloat))\n","\n","        # Trainable unregularised matrix that will be exponentiated to unitary\n","        self.U_param = nn.Parameter(torch.randn(matrix_size, matrix_size, dtype=torch.cfloat))\n","\n","        # Cached powers of  x  (x² … xᵈ)\n","        self.register_buffer(\"powers\", torch.zeros(d_order, matrix_size, matrix_size, dtype=torch.cfloat))\n","        self._powers_computed = False\n","\n","        # Placeholders for data tensors\n","        self.Mtr: torch.Tensor = None\n","        self.Mte: torch.Tensor = None\n","        self.y:   torch.Tensor = None\n","        self.yte: torch.Tensor = None\n","        self.target_matrices: torch.Tensor = None\n","\n","        self.to(self.device)\n","\n","    @staticmethod\n","    def _make_hermitian(M: torch.Tensor) -> torch.Tensor:\n","        return 0.5 * (M + M.conj().transpose(-2, -1))\n","\n","    @staticmethod\n","    def _frobenius_norm(mat: torch.Tensor) -> torch.Tensor:\n","        return torch.norm(mat, p=\"fro\", dim=(-2, -1), keepdim=True) + 1e-8\n","\n","    def _normalise(self, M: torch.Tensor) -> torch.Tensor:\n","        return M / self._frobenius_norm(M)\n","\n","    # def _make_unitary(self, matrix):\n","    #     U, _, Vh = torch.linalg.svd(matrix, full_matrices=False)\n","    #     return U @ Vh\n","\n","    # Other Methods of Enforcing Unitary COnstraints that can be tried...\n","    # def _make_unitary(self, matrix):\n","    #     Q, R = torch.linalg.qr(matrix)\n","    #     return Q\n","\n","    def _make_unitary(self, M: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Generates a unitary matrix through matrix exponential of a Hermitian:\n","            U = exp(-i H),  H = (M + M†)/2\n","        \"\"\"\n","        H = self._make_hermitian(M)\n","        return torch.matrix_exp(-1j * H)\n","\n","    @torch.no_grad()\n","    def _compute_powers(self) -> None:\n","        x_herm = self._make_hermitian(self._normalise(self.x))\n","\n","        x_power = x_herm @ x_herm\n","        self.powers[0] = x_power\n","        for k in range(1, self.d):\n","            x_power = x_power @ x_herm\n","            self.powers[k] = x_power\n","\n","        self._powers_computed = True\n","\n","    def _base_hamiltonian(self) -> torch.Tensor:\n","        x_h = self._make_hermitian(self._normalise(self.x))\n","        p_h = self._make_hermitian(self._normalise(self.p))\n","        return self._make_hermitian(0.5 * (p_h @ p_h) + 0.5 * (x_h @ x_h))\n","\n","    def _reconstruct_batch(self, coeffs: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Reconstruct Hamiltonians for a batch of coefficient vectors.\n","        coeffs.shape == (batch, d)\n","        \"\"\"\n","        if not self._powers_computed:\n","            self._compute_powers()\n","\n","        H0 = self._base_hamiltonian()\n","        batch = coeffs.shape[0]\n","\n","        recon = H0.unsqueeze(0).expand(batch, -1, -1).clone()\n","        for k in range(self.d):\n","            recon = recon + coeffs[:, k].unsqueeze(-1).unsqueeze(-1) * self.powers[k]\n","        recon = 0.5 * (recon + recon.conj().transpose(-2, -1))\n","        return recon\n","\n","    @staticmethod\n","    def _frobenius_batch(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n","        diff = A - B\n","        # ||A-B||_F² = Tr((A-B)†(A-B))\n","        tr = torch.diagonal(diff.conj().transpose(-2, -1) @ diff, dim1=-2, dim2=-1).sum(-1)\n","        return torch.abs(tr)  # shape: (batch,)\n","\n","    def load_data(\n","        self,\n","        train_mats: np.ndarray,\n","        train_labels: np.ndarray,\n","        test_mats: np.ndarray,\n","        test_labels: np.ndarray,\n","        target_mats: np.ndarray,\n","    ) -> None:\n","        print(f\"► Loading data on {self.device} …\")\n","\n","        self.Mtr  = torch.as_tensor(train_mats, dtype=torch.cfloat, device=self.device)\n","        self.y    = torch.as_tensor(train_labels, dtype=torch.long,  device=self.device)\n","        self.Mte  = torch.as_tensor(test_mats,  dtype=torch.cfloat, device=self.device)\n","        self.yte  = torch.as_tensor(test_labels, dtype=torch.long,  device=self.device)\n","        self.target_matrices = torch.as_tensor(target_mats, dtype=torch.cfloat, device=self.device)\n","\n","        self._compute_powers()  # pre-compute x-powers once\n","\n","        print(\n","            f\"✓ Data loaded — train {self.Mtr.shape}, test {self.Mte.shape}\"\n","        )\n","\n","    def forward(\n","        self,\n","        coeffs: torch.Tensor,             # (batch, d)\n","        originals: torch.Tensor,          # (batch, n, n)\n","        targets: torch.Tensor,            # (batch, n, n)\n","    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n","\n","        # 1 -  reconstruction loss\n","        reconstructed = self._reconstruct_batch(coeffs)\n","        L_recon = self._frobenius_batch(reconstructed, originals).mean()\n","\n","        # 2 -  unitary mapping loss\n","        U = self._make_unitary(self.U_param)\n","        transformed = U @ reconstructed @ U.conj().transpose(-2, -1)\n","        L_unitary = self._frobenius_batch(transformed, targets).mean()\n","\n","        # 3 -  unitary constraint\n","        I = torch.eye(self.n, dtype=torch.cfloat, device=self.device)\n","        L_uc0 = torch.norm(U @ U.conj().T - I, p=\"fro\")\n","        L_uc1 = torch.norm(U.conj().T @ U - I, p=\"fro\")\n","\n","        # weighted total\n","        total = L_recon + 10*L_unitary + 0.1 * (L_uc0 + L_uc1)\n","        return total, L_recon, L_unitary, L_uc0, L_uc1\n","\n","    @torch.no_grad()\n","    def accuracy(self, mats: torch.Tensor, labels: torch.Tensor) -> float:\n","        U = self._make_unitary(self.U_param)\n","        outs = U @ mats @ U.conj().transpose(-2, -1)\n","        diag = torch.abs(torch.diagonal(outs, dim1=-2, dim2=-1))      # (batch, n)\n","        preds = torch.argmax(diag, dim=-1) // 7                        # 10 classes (positions spaced by 7)\n","        return torch.mean((preds == labels).float()).item()\n","\n","class ComplexCoefficients(nn.Module):\n","    def __init__(self, d: int, n_samples: int, device: torch.device) -> None:\n","        super().__init__()\n","        std = 0.01 / np.sqrt(d)\n","        self.real = nn.Parameter(std * torch.randn(n_samples, d, device=device))\n","        self.imag = nn.Parameter(std * torch.randn(n_samples, d, device=device))\n","\n","    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n","        return torch.complex(self.real[idx], self.imag[idx])\n","\n","    # handy accessor\n","    def all(self) -> torch.Tensor:\n","        return torch.complex(self.real, self.imag)\n","\n","class ReduceLROnPlateau:\n","    def __init__(self, optimiser: optim.Optimizer, factor: float = 0.5, patience: int = 3, min_lr: float = 1e-8):\n","        self.opt = optimiser\n","        self.factor = factor\n","        self.patience = patience\n","        self.min_lr = min_lr\n","        self.best: float = None\n","        self.bad_epochs = 0\n","\n","    def step(self, metric: float):\n","        if self.best is None or metric < self.best:\n","            self.best = metric\n","            self.bad_epochs = 0\n","            return\n","        self.bad_epochs += 1\n","        if self.bad_epochs >= self.patience:\n","            for pg in self.opt.param_groups:\n","                new_lr = max(pg[\"lr\"] * self.factor, self.min_lr)\n","                pg[\"lr\"] = new_lr\n","            self.bad_epochs = 0\n","\n","\n","def create_target_matrices(labels: np.ndarray, matrix_size: int = 64) -> np.ndarray:\n","    positions = [0, 7, 14, 21, 28, 35, 42, 49, 56, 63]\n","    out = np.zeros((len(labels), matrix_size, matrix_size), dtype=np.complex64)\n","\n","    for i, lab in enumerate(labels):\n","        pos = positions[lab]\n","        out[i, pos, pos] = 1.0\n","\n","    for i in range(len(out)):\n","        norm = np.linalg.norm(out[i], 'fro')\n","        if norm > 0:\n","            out[i] = out[i] / norm\n","\n","    return out\n","\n","\n","def train_model(model: IntegratedHermitianClassifier) -> torch.Tensor:\n","    print(\"► Training started …\")\n","    t0 = time.time()\n","\n","    N = model.Mtr.shape[0]\n","    coeffs = ComplexCoefficients(model.d, N, model.device)\n","\n","    opt_coeff   = optim.Adam(coeffs.parameters(), lr=model.lr)\n","    opt_bases   = optim.Adam([model.x, model.p], lr=model.lr * 0.1)\n","    opt_unitary = optim.Adam([model.U_param],     lr=model.lr * 0.05)\n","\n","    sched_coeff   = ReduceLROnPlateau(opt_coeff)\n","    sched_bases   = ReduceLROnPlateau(opt_bases)\n","    sched_unitary = ReduceLROnPlateau(opt_unitary)\n","\n","    ds = TensorDataset(model.Mtr, model.target_matrices, torch.arange(N, device=model.device))\n","    dl = DataLoader(ds, batch_size=model.batch_size, shuffle=True)\n","\n","    for epoch in range(1, model.epochs + 1):\n","        print(f\"Epoch: {epoch}\")\n","        loss_sum = recon_sum = unit_sum = 0.0\n","        batches = 0\n","\n","        for orig, tgt, idx in dl:\n","            a = coeffs(idx)\n","\n","            opt_coeff.zero_grad()\n","            opt_bases.zero_grad()\n","            opt_unitary.zero_grad()\n","\n","            tot, Lr, Lu, _, _ = model(a, orig, tgt)\n","            tot.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(coeffs.parameters(), 1.0)\n","            torch.nn.utils.clip_grad_norm_([model.x, model.p], 0.5)\n","            torch.nn.utils.clip_grad_norm_([model.U_param], 0.5)\n","\n","            opt_coeff.step()\n","            opt_bases.step()\n","            opt_unitary.step()\n","\n","            model._compute_powers()\n","\n","            loss_sum  += tot.item()\n","            recon_sum += Lr.item()\n","            unit_sum  += Lu.item()\n","            batches   += 1\n","\n","        sched_coeff.step(loss_sum / batches)\n","        sched_bases.step(recon_sum / batches)\n","        sched_unitary.step(unit_sum / batches)\n","\n","        train_acc = model.accuracy(model.Mtr, model.y)\n","        test_acc  = model.accuracy(model.Mte, model.yte)\n","\n","        print(\n","            f\"  Epoch {epoch:02d}/{model.epochs} | \"\n","            f\"Loss {loss_sum / batches:.4e} | \"\n","            f\"Recon {recon_sum / batches:.4e} | \"\n","            f\"Unit {unit_sum / batches:.4e} | \"\n","            f\"Acc train {train_acc:.3f} │ test {test_acc:.3f} | \"\n","            f\"Δt {time.time()-t0:.1f}s\"\n","        )\n","\n","    print(f\"✓ Training finished in {time.time() - t0:.1f}s\")\n","    return coeffs.all()\n","\n","\n","# @torch.no_grad()\n","def predict_coefficients(\n","    model: IntegratedHermitianClassifier,\n","    d_order: int,\n","    lr: float = 1e-3,\n","    epochs: int = 200,\n","    batch_size: int = 2000,\n",") -> torch.Tensor:\n","    print(\"► Predicting coefficients for test set …\")\n","    Nt = model.Mte.shape[0]\n","    out = torch.empty((Nt, d_order), dtype=torch.cfloat, device=model.device)\n","\n","    ds = TensorDataset(model.Mte, torch.arange(Nt, device=model.device))\n","    dl = DataLoader(ds, batch_size=batch_size)\n","\n","    for mats, idx in dl:\n","        coeff = ComplexCoefficients(d_order, len(idx), model.device)\n","        opt = optim.Adam(coeff.parameters(), lr=lr)\n","\n","        for _ in range(epochs):\n","            opt.zero_grad()\n","            preds = coeff(torch.arange(len(idx), device=model.device))\n","            recon = model._reconstruct_batch(preds)\n","            loss = model._frobenius_batch(recon, mats).mean()\n","            loss.backward()\n","            opt.step()\n","\n","        out[idx] = coeff.all()\n","    print(\"✓ Coefficient prediction complete\")\n","    return out\n","\n","def run_pipeline(\n","    train_mats: np.ndarray,\n","    train_labels: np.ndarray,\n","    test_mats: np.ndarray,\n","    test_labels: np.ndarray,\n","    *,\n","    matrix_size: int = 64,\n","    d_order: int = 10,\n","    lr: float = 5e-3,\n","    epochs: int = 50,\n","    batch_size: int = 128,\n","    out_dir: str = \"./\",\n",") -> Tuple[IntegratedHermitianClassifier, torch.Tensor, torch.Tensor]:\n","\n","    set_seed(42)\n","\n","    # Build class-specific target Hamiltonians\n","    targets = create_target_matrices(train_labels, matrix_size)\n","\n","    # Initialise model and load data\n","    model = IntegratedHermitianClassifier(\n","        matrix_size=matrix_size,\n","        d_order=d_order,\n","        lr=lr,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","    )\n","    model.load_data(train_mats, train_labels, test_mats, test_labels, targets)\n","\n","    # Train\n","    train_coeffs = train_model(model)\n","\n","    # Predict test coefficients\n","    #test_coeffs = predict_coefficients(model, d_order, lr=lr * 0.1)\n","\n","    # # Persist artefacts\n","    # save_results(train_coeffs, test_coeffs, model, out_dir)\n","\n","    # Final accuracies\n","    print(\"\\n================ FINAL ACCURACY ================ \")\n","    print(f\"Train: {model.accuracy(model.Mtr, model.y):.3%}\")\n","    print(f\" Test: {model.accuracy(model.Mte, model.yte):.3%}\")\n","    print(\"===============================================\\n\")\n","\n","    return model, train_coeffs, test_coeffs\n","\n","\n","def create_labels_from_class_counts(class_counts):\n","    labels = []\n","    for class_idx, count in enumerate(class_counts):\n","        labels.extend([class_idx] * count)\n","    return labels\n","\n","class_counts = [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n","\n","train_X = normalized_Hermitian_Digit_matrices.numpy()\n","train_y = create_labels_from_class_counts(class_counts)\n","test_X = normalized_hermitian_matrices_test_input.numpy()\n","test_y = y_test\n","\n","run_pipeline(train_X, train_y, test_X, test_y, matrix_size = 64, d_order = 50, lr = 1e-1, epochs = 60, batch_size = 100)"],"metadata":{"id":"Zvs_0eP_jkey"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Case 2:-**\n","\n","Thsi is robust improvement done over case 1.Here instead of one global set of ciefficients, here the model learns 10 seperate pairs of coefficients per class and hence model trains class specific base Hamiltonians with training objective of minimizing the combined loss from reconstruction and unitary mapping.The classification is done on class to class basis with new set of coefficients being learnt per class and then calculating 2 erros: reconstruction error and target loss and then first classes with low targrt loss are chosen after which the class with least reconstruction loss is chosen."],"metadata":{"id":"NvJFxOLIj9_7"}},{"cell_type":"code","source":["from __future__ import annotations\n","import time\n","from typing import Tuple, List\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","def set_seed(seed: int = 42) -> None:\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","\n","\n","class IntegratedHermitianClassifier(nn.Module):\n","    def __init__(\n","        self,\n","        matrix_size: int = 64,\n","        n_classes: int = 10,\n","        d_order: int = 10,\n","        lr: float = 2e-3,\n","        epochs: int = 100,\n","        batch_size: int = 512,\n","        device: str | None = None,\n","    ) -> None:\n","        super().__init__()\n","\n","        self.n, self.C, self.d = matrix_size, n_classes, d_order\n","        self.lr, self.epochs, self.batch_size = lr, epochs, batch_size\n","        self.device = torch.device(device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n","\n","        # class-wise Hermitian bases  (x_c , p_c)  → Parameters (C,n,n)\n","        self.x_cls = nn.Parameter(0.1 * torch.randn(self.C, self.n, self.n, dtype=torch.cfloat))\n","        self.p_cls = nn.Parameter(0.1 * torch.randn(self.C, self.n, self.n, dtype=torch.cfloat))\n","\n","        # global unconstrained parameter whose exponential is unitary\n","        self.U_param = nn.Parameter(torch.randn(self.n, self.n, dtype=torch.cfloat))\n","\n","        # data placeholders\n","        self.Mtr = self.Mte = self.y = self.yte = self.target_mats = None\n","\n","        self.to(self.device)\n","\n","    @staticmethod\n","    def _make_hermitian(M: torch.Tensor) -> torch.Tensor:\n","        return 0.5 * (M + M.conj().transpose(-2, -1))\n","\n","    @staticmethod\n","    def _fro(M: torch.Tensor) -> torch.Tensor:\n","        return torch.norm(M, p=\"fro\", dim=(-2, -1), keepdim=True) + 1e-8\n","\n","    def _unitary(self) -> torch.Tensor:\n","        H = self._make_hermitian(self.U_param)\n","        return torch.matrix_exp(-1j * H)\n","\n","    def _reconstruct(\n","        self,\n","        coeffs: torch.Tensor,      # (B,d)\n","        x_b:   torch.Tensor,       # (B,n,n)  — already class-selected\n","        p_b:   torch.Tensor,       # (B,n,n)\n","    ) -> torch.Tensor:\n","        \"\"\"Vectorised reconstruction for a batch.\"\"\"\n","        B = coeffs.shape[0]\n","        x_h = self._make_hermitian(self._normalise(x_b := self._normalise(x_b)))\n","        p_h = self._make_hermitian(self._normalise(p_b := self._normalise(p_b)))\n","\n","        H0 = self._make_hermitian(0.5 * (p_h @ p_h) + 0.5 * (x_h @ x_h))\n","        powers = []\n","        x_pow = x_h @ x_h\n","        powers.append(x_pow)\n","        for _ in range(1, self.d):\n","            x_pow = x_pow @ x_h\n","            powers.append(x_pow)\n","\n","        recon = H0.clone()\n","        for k in range(self.d):\n","            recon = recon + coeffs[:, k].view(B, 1, 1) * powers[k]\n","        recon = 0.5 * (recon + recon.conj().transpose(-2, -1))\n","        return recon\n","\n","    def _normalise(self, M: torch.Tensor) -> torch.Tensor:\n","        return M / self._fro(M)\n","\n","    def forward(\n","        self,\n","        coeffs: torch.Tensor,      # (B,d)\n","        labels: torch.Tensor,      # (B,)\n","        originals: torch.Tensor,   # (B,n,n)\n","        targets: torch.Tensor,     # (B,n,n)\n","    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n","\n","        x_b = self.x_cls[labels]\n","        p_b = self.p_cls[labels]\n","        recon = self._reconstruct(coeffs, x_b, p_b)\n","\n","        diff     = recon - originals\n","        L_recon  = torch.diagonal(diff.conj().transpose(-2, -1) @ diff,\n","                                  dim1=-2, dim2=-1).sum(-1).abs().mean()\n","\n","        U  = self._unitary()\n","        Uh = U.conj().transpose(-2, -1)\n","        diffU = U @ recon @ Uh - targets\n","        L_unit = torch.diagonal(diffU.conj().transpose(-2, -1) @ diffU,\n","                                dim1=-2, dim2=-1).sum(-1).abs().mean()\n","\n","        I = torch.eye(self.n, dtype=torch.cfloat, device=self.device)\n","        L_uc = torch.norm(U @ Uh - I, p=\"fro\") + torch.norm(Uh @ U - I, p=\"fro\")\n","\n","        total = L_recon + L_unit #+ 0.1 * L_uc\n","        return total, L_recon, L_unit, L_uc\n","\n","    @torch.no_grad()\n","    def accuracy_diag(self, mats: torch.Tensor, labels: torch.Tensor) -> float:\n","        \"\"\"Quick diagnostic accuracy – diag( UHU† ).\"\"\"\n","        U = self._unitary()\n","        outs = U @ mats @ U.conj().transpose(-2, -1)\n","        preds = torch.diagonal(outs.abs(), dim1=-2, dim2=-1).argmax(-1) // 7\n","        return (preds == labels).float().mean().item()\n","\n","    def load_data(\n","        self,\n","        train_mats: np.ndarray,\n","        train_labels: np.ndarray,\n","        test_mats:  np.ndarray,\n","        test_labels: np.ndarray,\n","        target_mats: np.ndarray,\n","    ) -> None:\n","        print(f\"► Loading data on {self.device} …\")\n","\n","        self.Mtr = torch.tensor(train_mats, dtype=torch.cfloat, device=self.device)\n","        self.y   = torch.tensor(train_labels, dtype=torch.long, device=self.device)\n","        self.Mte = torch.tensor(test_mats,  dtype=torch.cfloat, device=self.device)\n","        self.yte = torch.tensor(test_labels, dtype=torch.long, device=self.device)\n","        self.target_mats = torch.tensor(target_mats, dtype=torch.cfloat, device=self.device)\n","\n","        print(f\"Data loaded — train {self.Mtr.shape}, test {self.Mte.shape}\")\n","\n","\n","class ComplexCoefficients(nn.Module):\n","    def __init__(self, d: int, n_samples: int, device: torch.device):\n","        super().__init__()\n","        std = 0.01 / np.sqrt(d)\n","        self.real = nn.Parameter(std * torch.randn(n_samples, d, device=device))\n","        self.imag = nn.Parameter(std * torch.randn(n_samples, d, device=device))\n","\n","    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n","        return torch.complex(self.real[idx], self.imag[idx])\n","\n","    def all(self) -> torch.Tensor:\n","        return torch.complex(self.real, self.imag)\n","\n","class ReduceLROnPlateau:\n","    def __init__(self, opt: optim.Optimizer, factor=.5, patience=3, min_lr=1e-8):\n","        self.opt, self.factor, self.patience, self.min_lr = opt, factor, patience, min_lr\n","        self.best, self.bad = None, 0\n","\n","    def step(self, metric: float):\n","        if self.best is None or metric < self.best:\n","            self.best, self.bad = metric, 0\n","            return\n","        self.bad += 1\n","        if self.bad >= self.patience:\n","            for pg in self.opt.param_groups:\n","                pg[\"lr\"] = max(pg[\"lr\"] * self.factor, self.min_lr)\n","            self.bad = 0\n","\n","def create_target_matrices(labels: np.ndarray | List[int], n: int = 64) -> np.ndarray:\n","    pos = [0, 7, 14, 21, 28, 35, 42, 49, 56, 63]\n","    out = np.zeros((len(labels), n, n), np.complex64)\n","    for i, lab in enumerate(labels):\n","        out[i, pos[lab], pos[lab]] = 1.0\n","    out /= np.linalg.norm(out, axis=(-2, -1), keepdims=True, ord='fro')\n","    return out\n","\n","\n","def train(model: IntegratedHermitianClassifier) -> torch.Tensor:\n","    print(\"► Training …\")\n","    t0 = time.time()\n","\n","    N = model.Mtr.shape[0]\n","    coeffs = ComplexCoefficients(model.d, N, model.device)\n","\n","    opt_coeff = optim.Adam(coeffs.parameters(), lr=model.lr)\n","    opt_bases = optim.Adam([model.x_cls, model.p_cls], lr=model.lr * .1)\n","    opt_unit  = optim.Adam([model.U_param],             lr=model.lr * .05)\n","\n","    sch_coeff = ReduceLROnPlateau(opt_coeff)\n","    sch_bases = ReduceLROnPlateau(opt_bases)\n","    sch_unit  = ReduceLROnPlateau(opt_unit)\n","\n","    ds = TensorDataset(model.Mtr, model.target_mats, model.y, torch.arange(N, device=model.device))\n","    dl = DataLoader(ds, batch_size=model.batch_size, shuffle=True)\n","\n","    for ep in range(1, model.epochs + 1):\n","        print(f\"Epoch: {ep}\")\n","        tot_sum = rec_sum = unit_sum = 0.0; batches = 0\n","        for H, T, lab, idx in dl:\n","            a = coeffs(idx)\n","\n","            opt_coeff.zero_grad(); opt_bases.zero_grad(); opt_unit.zero_grad()\n","            tot, Lr, Lu, L_uc = model(a, lab, H, T)\n","            tot.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(coeffs.parameters(), 1.0)\n","            torch.nn.utils.clip_grad_norm_([model.x_cls, model.p_cls], .5)\n","            torch.nn.utils.clip_grad_norm_([model.U_param], .5)\n","\n","            opt_coeff.step(); opt_bases.step(); opt_unit.step()\n","\n","            tot_sum += tot.item(); rec_sum += Lr.item(); unit_sum += Lu.item()\n","            batches += 1\n","\n","        sch_coeff.step(tot_sum / batches)\n","        sch_bases.step(rec_sum / batches)\n","        sch_unit.step(unit_sum / batches)\n","\n","        print(f\"  Ep {ep:03d}/{model.epochs} | \"\n","              f\"Loss {tot_sum/batches:.4e} | \"\n","              f\"Recon {rec_sum/batches:.4e} | \"\n","              f\"Unit {unit_sum/batches:.4e} | \"\n","              f\"Acc {model.accuracy_diag(model.Mtr, model.y):.3f} │ \"\n","              f\"Δt {time.time()-t0:.1f}s\")\n","\n","    print(\"Done  (training time {:.1f}s)\".format(time.time()-t0))\n","    return coeffs.all()\n","\n","\n","def classify_test_set(\n","    model: IntegratedHermitianClassifier,\n","    d_order: int,\n","    lr: float = 1e-3,\n","    epochs: int = 100,\n","    batch_size: int = 10000,\n","    tol_target: float = 1e-3,\n",") -> Tuple[torch.Tensor, torch.Tensor]:\n","    \"\"\"\n","    Returns\n","    -------\n","    coeffs_out : (N_test,d)  coefficients of the chosen class per sample\n","    preds      : (N_test,)   predicted class labels\n","    \"\"\"\n","    print(\"Classifying test set …\")\n","    N = model.Mte.shape[0]\n","    coeffs_out = torch.empty((N, d_order), dtype=torch.cfloat, device=model.device)\n","    preds      = torch.empty(N, dtype=torch.long, device=model.device)\n","\n","    # pre-compute class targets (C,n,n)\n","    tgt_cls = torch.tensor(create_target_matrices(list(range(model.C)), model.n),\n","                           dtype=torch.cfloat, device=model.device)\n","    U  = model._unitary()\n","    Uh = U.conj().transpose(-2, -1)\n","\n","    ds = TensorDataset(model.Mte, torch.arange(N, device=model.device))\n","    dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n","\n","    for H_batch, idx_batch in dl:\n","        for j, idx in enumerate(idx_batch):\n","            H = H_batch[j:j+1]\n","\n","            best_match_loss = torch.inf\n","            best_nomatch_loss = torch.inf\n","            best_coeff = None\n","            best_class = None\n","            matched = False\n","\n","            for c in range(model.C):\n","                coeff = ComplexCoefficients(d_order, 1, model.device)\n","                opt   = optim.Adam(coeff.parameters(), lr=lr)\n","\n","                x_c = model.x_cls[c:c+1]  # (1,n,n)\n","                p_c = model.p_cls[c:c+1]\n","\n","                for _ in range(epochs):\n","                    opt.zero_grad()\n","                    a = coeff(torch.tensor([0], device=model.device))\n","                    recon = model._reconstruct(a, x_c, p_c)\n","                    loss = (recon - H).abs().pow(2).sum()\n","                    loss.backward()\n","                    opt.step()\n","\n","                a_opt = coeff.all()  # (1,d)\n","                recon = model._reconstruct(a_opt, x_c, p_c)\n","                rec_loss = (recon - H).abs().pow(2).sum().item()\n","\n","                out = U @ recon @ Uh\n","                tgt_diff = (out - tgt_cls[c:c+1]).abs().pow(2).sum().item()\n","\n","                if tgt_diff < tol_target:      # criterion 1 satisfied\n","                    matched = True\n","                    if rec_loss < best_match_loss:\n","                        best_match_loss = rec_loss\n","                        best_coeff, best_class = a_opt.squeeze(0), c\n","                elif not matched and rec_loss < best_nomatch_loss:\n","                    best_nomatch_loss = rec_loss\n","                    best_coeff, best_class = a_opt.squeeze(0), c\n","\n","            coeffs_out[idx] = best_coeff\n","            preds[idx]      = best_class\n","\n","    print(\"Classification complete\")\n","    return coeffs_out, preds\n","\n","\n","def run_pipeline(\n","    train_mats: np.ndarray,\n","    train_labels: np.ndarray,\n","    test_mats:  np.ndarray,\n","    test_labels: np.ndarray,\n","    *,\n","    matrix_size: int = 64,\n","    d_order: int = 10,\n","    lr: float = 5e-3,\n","    epochs: int = 200,\n","    batch_size: int = 1024,\n",") -> Tuple[IntegratedHermitianClassifier, torch.Tensor, torch.Tensor, torch.Tensor]:\n","\n","    set_seed(42)\n","\n","    tgt_train = create_target_matrices(train_labels, matrix_size)\n","\n","    model = IntegratedHermitianClassifier(\n","        matrix_size=matrix_size,\n","        d_order=d_order,\n","        lr=lr,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","    )\n","    model.load_data(train_mats, train_labels, test_mats, test_labels, tgt_train)\n","\n","    coeff_tr = train(model)\n","    #coeff_te, preds_te = classify_test_set(model, d_order, lr=lr*0.1)\n","\n","    print(f\" Train diag-acc  : {model.accuracy_diag(model.Mtr, model.y):.3%}\")\n","    print(f\" Test  diag-acc  : {model.accuracy_diag(model.Mte, model.yte):.3%}\")\n","    #print(f\" Test  class-acc : {(preds_te.cpu().numpy()==test_labels).mean():.3%}\")\n","\n","    return model, coeff_tr, coeff_te, preds_te\n","\n","\n","def create_labels_from_class_counts(class_counts):\n","    labels = []\n","    for class_idx, count in enumerate(class_counts):\n","        labels.extend([class_idx] * count)\n","    return labels\n","\n","class_counts = [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n","\n","train_X = normalized_Hermitian_Digit_matrices.numpy()\n","train_y = create_labels_from_class_counts(class_counts)\n","test_X = normalized_hermitian_matrices_test_input.numpy()\n","test_y = y_test\n","\n","run_pipeline(train_X, train_y, test_X, test_y, matrix_size = 64, d_order = 90, lr = 1e-1, epochs = 60, batch_size = 100)\n","\n"],"metadata":{"id":"8XsG9mLslF29"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Case 1 + Case 5**"],"metadata":{"id":"xPYS9xNSlJv0"}},{"cell_type":"markdown","source":["This an improved extension of case 1 in which we have used used the smae coefficients for all classes but different unitary matrices for classes."],"metadata":{"id":"oQIz1U0xlQ79"}},{"cell_type":"code","source":["# x and p shared for separate classes , U seperate for seperate classes...(OVR Strategy) - Case 1 plus Case 5\n","\n","from __future__ import annotations\n","import time\n","from typing import Tuple, List\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","def set_seed(seed: int = 42) -> None:\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","\n","def create_target_matrices(labels: np.ndarray | List[int], n: int = 64) -> np.ndarray:\n","    \"\"\"Create diagonal target matrices with class-specific positions.\"\"\"\n","    pos = [0, 7, 14, 21, 28, 35, 42, 49, 56, 63]\n","    out = np.zeros((len(labels), n, n), np.complex64)\n","    for i, lab in enumerate(labels):\n","        out[i, pos[lab], pos[lab]] = 1.0\n","    norms = np.linalg.norm(out, axis=(-2, -1), ord='fro', keepdims=True)\n","    out = out / (norms + 1e-8)\n","    return out\n","\n","class ComplexCoefficients(nn.Module):\n","    def __init__(self, d: int, n_samples: int, device: torch.device):\n","        super().__init__()\n","        std = 0.01 / np.sqrt(d)\n","        self.real = nn.Parameter(std * torch.randn(n_samples, d, device=device))\n","        self.imag = nn.Parameter(std * torch.randn(n_samples, d, device=device))\n","\n","    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n","        return torch.complex(self.real[idx], self.imag[idx])\n","\n","    def all(self) -> torch.Tensor:\n","        return torch.complex(self.real, self.imag)\n","\n","class ReduceLROnPlateau:\n","    \"\"\"Simple learning rate scheduler.\"\"\"\n","    def __init__(self, opt: optim.Optimizer, factor=0.5, patience=3, min_lr=1e-8):\n","        self.opt, self.factor, self.patience, self.min_lr = opt, factor, patience, min_lr\n","        self.best, self.bad = None, 0\n","\n","    def step(self, metric: float):\n","        if self.best is None or metric < self.best:\n","            self.best, self.bad = metric, 0\n","            return\n","        self.bad += 1\n","        if self.bad >= self.patience:\n","            for pg in self.opt.param_groups:\n","                pg[\"lr\"] = max(pg[\"lr\"] * self.factor, self.min_lr)\n","            self.bad = 0\n","\n","class OvRMultiMarginHermitianClassifier(nn.Module):\n","    \"\"\"\n","    Multi-class classifier with:\n","    - Shared Hermitian bases (x, p) for all classes\n","    - Separate unitary matrices U_k for each class (OvR strategy)\n","    - Multi-margin hinge loss objective\n","    \"\"\"\n","    def __init__(\n","        self,\n","        matrix_size: int = 64,\n","        n_classes: int = 10,\n","        d_order: int = 10,\n","        lr: float = 2e-3,\n","        epochs: int = 100,\n","        batch_size: int = 128,\n","        chunk_size: int = 100,\n","        margin: float = 1.0,\n","        device: str | None = None,\n","    ) -> None:\n","        super().__init__()\n","\n","        self.n, self.C, self.d = matrix_size, n_classes, d_order\n","        self.lr, self.epochs, self.batch_size = lr, epochs, batch_size\n","        self.chunk_size = chunk_size\n","        self.margin = margin\n","        self.device = torch.device(device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n","\n","        # Shared Hermitian bases (x, p), parameters (n, n)\n","        self.x_mat = nn.Parameter(0.1 * torch.randn(self.n, self.n, dtype=torch.cfloat,device = self.device))\n","        self.p_mat = nn.Parameter(0.1 * torch.randn(self.n, self.n, dtype=torch.cfloat,device = self.device))\n","\n","        # Separate unitary parameters for each class (C, n, n)\n","        self.U_param = nn.Parameter(torch.randn(self.C, self.n, self.n, dtype=torch.cfloat,device = self.device))\n","\n","        # Class-specific target matrices (C, n, n)\n","        class_targets = create_target_matrices(list(range(self.C)), self.n)\n","        self.class_targets = torch.tensor(class_targets, dtype=torch.cfloat, device=self.device)\n","\n","        # Data placeholders\n","        self.Mtr = self.Mte = self.y = self.yte = self.target_mats = None\n","\n","        self.to(self.device)\n","\n","    @staticmethod\n","    def _make_hermitian(M: torch.Tensor) -> torch.Tensor:\n","        return 0.5 * (M + M.conj().transpose(-2, -1))\n","\n","    @staticmethod\n","    def _fro_norm(M: torch.Tensor) -> torch.Tensor:\n","        return torch.norm(M, p=\"fro\", dim=(-2, -1), keepdim=True) + 1e-8\n","\n","    def _normalise(self, M: torch.Tensor) -> torch.Tensor:\n","        return M / self._fro_norm(M)\n","\n","    def _make_unitary(self, M: torch.Tensor) -> torch.Tensor:\n","        H = self._make_hermitian(M)\n","        return torch.matrix_exp(-1j * H)\n","\n","    def _reconstruct(\n","        self,\n","        coeffs: torch.Tensor,      # (B, d)\n","        x_b: torch.Tensor,         # (B, n, n) or (n, n) if shared\n","        p_b: torch.Tensor,         # (B, n, n) or (n, n) if shared\n","    ) -> torch.Tensor:\n","        \"\"\"Vectorized reconstruction for a batch.\"\"\"\n","        B = coeffs.shape[0]\n","\n","        x_h = self._make_hermitian(self._normalise(x_b))\n","        p_h = self._make_hermitian(self._normalise(p_b))\n","\n","        # Base Hamiltonian: H0 = 0.5*(p²) + 0.5*(x²)\n","        H0 = self._make_hermitian(0.5 * (p_h @ p_h) + 0.5 * (x_h @ x_h))\n","\n","        # Compute powers of x\n","        powers = []\n","        x_pow = x_h @ x_h\n","        powers.append(x_pow)\n","        for _ in range(1, self.d):\n","            x_pow = x_pow @ x_h\n","            powers.append(x_pow)\n","\n","        # Reconstruct: H = H0 + Σ a_k * x^{k+2}\n","        recon = H0.clone()\n","        for k in range(self.d):\n","            recon = recon + coeffs[:, k].view(B, 1, 1) * powers[k]\n","\n","        recon = 0.5 * (recon + recon.conj().transpose(-2, -1))\n","        return recon\n","\n","    def forward(\n","        self,\n","        coeffs: torch.Tensor,      # (B, d)\n","        labels: torch.Tensor,      # (B,)\n","        originals: torch.Tensor,   # (B, n, n)\n","    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n","\n","        B = originals.size(0)\n","        # Shared x and p for all classes, expand to (B, n, n)\n","        x_b = self.x_mat.unsqueeze(0).expand(B, self.n, self.n)\n","        p_b = self.p_mat.unsqueeze(0).expand(B, self.n, self.n)\n","\n","        # Reconstruct Hamiltonians\n","        recon = self._reconstruct(coeffs, x_b, p_b)\n","\n","        # 1) Reconstruction loss\n","        diff = recon - originals\n","        L_recon = torch.diagonal(diff.conj().transpose(-2, -1) @ diff, dim1=-2, dim2=-1).sum(-1).abs().mean()\n","\n","        # 2) Multi-margin hinge loss with separate unitaries\n","        U = self._make_unitary(self.U_param)  # (C, n, n)\n","\n","        # Compute scores for all classes (memory-efficient)\n","        scores = torch.zeros(B, self.C, device=self.device)\n","        for k in range(self.C):\n","            U_k = U[k]\n","            U_k_dag = U_k.conj().T\n","            target_k = self.class_targets[k]\n","            transformed_k = U_k @ recon @ U_k_dag\n","            dists_k = torch.norm(transformed_k - target_k.unsqueeze(0), p=\"fro\", dim=(-2, -1))\n","            scores[:, k] = -dists_k\n","\n","        # Multi-margin hinge loss: L = (1/C) * Σ_{j≠i} max(0, margin + s_j - s_i)\n","        correct_scores = scores[torch.arange(B), labels]\n","        hinge_losses = torch.zeros(B, device=self.device)\n","        for i in range(B):\n","            correct_score = correct_scores[i]\n","            wrong_scores = torch.cat([scores[i, :labels[i]], scores[i, labels[i]+1:]])\n","            margins = torch.clamp(self.margin + wrong_scores - correct_score, min=0.0)\n","            hinge_losses[i] = margins.mean()\n","        L_multimargin = hinge_losses.mean()\n","\n","        # 3) Unitary constraints (batched)\n","        I = torch.eye(self.n, dtype=torch.cfloat, device=self.device)\n","        U_dag = U.conj().transpose(-2, -1)\n","        L_uc = torch.norm(U @ U_dag - I, p=\"fro\") + torch.norm(U_dag @ U - I, p=\"fro\")\n","\n","        # Total loss\n","        total = L_recon + 5.0 * L_multimargin + 0.1 * L_uc\n","        return total, L_recon, L_multimargin, L_uc\n","\n","    @torch.no_grad()\n","    def accuracy(self, mats: torch.Tensor, labels: torch.Tensor) -> float:\n","        \"\"\"Memory-efficient accuracy computation using chunking.\"\"\"\n","        U = self._make_unitary(self.U_param)  # (C, n, n)\n","\n","        total_correct = 0\n","        total_samples = 0\n","\n","        for start_idx in range(0, mats.size(0), self.chunk_size):\n","            end_idx = min(start_idx + self.chunk_size, mats.size(0))\n","            chunk_mats = mats[start_idx:end_idx]\n","            chunk_labels = labels[start_idx:end_idx]\n","\n","            chunk_size_actual = chunk_mats.size(0)\n","            scores = torch.zeros(chunk_size_actual, self.C, device=self.device)\n","\n","            for k in range(self.C):\n","                U_k = U[k]\n","                U_k_dag = U_k.conj().T\n","                target_k = self.class_targets[k]\n","\n","                transformed_k = U_k @ chunk_mats @ U_k_dag\n","                dists_k = torch.norm(transformed_k - target_k.unsqueeze(0), p=\"fro\", dim=(-2, -1))\n","                scores[:, k] = -dists_k\n","\n","            preds = torch.argmax(scores, dim=1)\n","            total_correct += (preds == chunk_labels).sum().item()\n","            total_samples += chunk_size_actual\n","\n","        return total_correct / total_samples\n","\n","    def load_data(\n","        self,\n","        train_mats: np.ndarray,\n","        train_labels: np.ndarray,\n","        test_mats: np.ndarray,\n","        test_labels: np.ndarray,\n","        target_mats: np.ndarray,\n","    ) -> None:\n","        print(f\"Loading data on {self.device} …\")\n","        self.Mtr = torch.tensor(train_mats, dtype=torch.cfloat, device=self.device)\n","        self.y   = torch.tensor(train_labels, dtype=torch.long, device=self.device)\n","        self.Mte = torch.tensor(test_mats, dtype=torch.cfloat, device=self.device)\n","        self.yte = torch.tensor(test_labels, dtype=torch.long, device=self.device)\n","        self.target_mats = torch.tensor(target_mats, dtype=torch.cfloat, device=self.device)\n","        print(f\"Data loaded — train {self.Mtr.shape}, test {self.Mte.shape}\")\n","\n","def train_model(model: OvRMultiMarginHermitianClassifier) -> torch.Tensor:\n","    \"\"\"Training routine with multi-margin hinge loss.\"\"\"\n","    print(\"► Training started …\")\n","    t0 = time.time()\n","\n","    N = model.Mtr.shape[0]\n","    coeffs = ComplexCoefficients(model.d, N, model.device)\n","\n","    # Optimizers with different learning rates\n","    opt_coeff = optim.Adam(coeffs.parameters(), lr=model.lr)\n","    opt_bases = optim.Adam([model.x_mat, model.p_mat], lr=model.lr * 0.1)\n","    opt_units = optim.Adam([model.U_param], lr=model.lr * 0.05)\n","\n","    # Schedulers\n","    sch_coeff = ReduceLROnPlateau(opt_coeff)\n","    sch_bases = ReduceLROnPlateau(opt_bases)\n","    sch_units = ReduceLROnPlateau(opt_units)\n","\n","    # DataLoader\n","    ds = TensorDataset(model.Mtr, model.target_mats, model.y, torch.arange(N, device=model.device))\n","    dl = DataLoader(ds, batch_size=model.batch_size, shuffle=True)\n","\n","    for ep in range(1, model.epochs + 1):\n","        print(f\"Epoch:{ep}\")\n","        tot_sum = rec_sum = margin_sum = 0.0\n","        batches = 0\n","\n","        for H, T, lab, idx in dl:\n","            H, T, lab, idx = H.to(model.device), T.to(model.device), lab.to(model.device), idx.to(model.device)\n","            a = coeffs(idx)\n","            opt_coeff.zero_grad()\n","            opt_bases.zero_grad()\n","            opt_units.zero_grad()\n","            tot, L_recon, L_margin, L_uc = model(a, lab, H)\n","            tot.backward()\n","            torch.nn.utils.clip_grad_norm_(coeffs.parameters(), 1.0)\n","            torch.nn.utils.clip_grad_norm_([model.x_mat, model.p_mat], 0.5)\n","            torch.nn.utils.clip_grad_norm_([model.U_param], 0.5)\n","            opt_coeff.step()\n","            opt_bases.step()\n","            opt_units.step()\n","            tot_sum += tot.item()\n","            rec_sum += L_recon.item()\n","            margin_sum += L_margin.item()\n","            batches += 1\n","            del H, T, lab, idx\n","            torch.cuda.empty_cache()\n","\n","        sch_coeff.step(tot_sum / batches)\n","        sch_bases.step(rec_sum / batches)\n","        sch_units.step(margin_sum / batches)\n","\n","        if ep % 1 == 0 or ep == 1:\n","            train_acc = model.accuracy(model.Mtr, model.y)\n","            test_acc = model.accuracy(model.Mte, model.yte)\n","            acc_str = f\"Acc train {train_acc:.3f} │ test {test_acc:.3f} | \"\n","        else:\n","            acc_str = \"\"\n","\n","        print(f\"  Ep {ep:03d}/{model.epochs} | \"\n","              f\"Loss {tot_sum/batches:.4e} | \"\n","              f\"Recon {rec_sum/batches:.4e} | \"\n","              f\"Margin {margin_sum/batches:.4e} | \"\n","              f\"{acc_str}\"\n","              f\"Δt {time.time()-t0:.1f}s\")\n","\n","    print(f\"Training finished in {time.time() - t0:.1f}s\")\n","    return coeffs.all()\n","\n","def classify_test_set(\n","    model: OvRMultiMarginHermitianClassifier,\n","    d_order: int,\n","    lr: float = 1e-3,\n","    epochs: int = 100,\n","    batch_size: int = 1000,\n","    tol_target: float = 1e-3,\n",") -> Tuple[torch.Tensor, torch.Tensor]:\n","    \"\"\"\n","    Classify test samples by finding best reconstruction for each class.\n","    Returns:\n","        coeffs_out: (N_test, d) coefficients of the chosen class per sample\n","        preds: (N_test,) predicted class labels\n","    \"\"\"\n","    print(\"Classifying test set …\")\n","    N = model.Mte.shape[0]\n","    coeffs_out = torch.empty((N, d_order), dtype=torch.cfloat, device=model.device)\n","    preds = torch.empty(N, dtype=torch.long, device=model.device)\n","\n","    # Pre-compute unitaries and targets\n","    U = model._make_unitary(model.U_param)\n","\n","    ds = TensorDataset(model.Mte, torch.arange(N, device=model.device))\n","    dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n","\n","    for H_batch, idx_batch in dl:\n","        for j, idx in enumerate(idx_batch):\n","            H = H_batch[j:j+1]\n","\n","            best_match_loss = torch.inf\n","            best_nomatch_loss = torch.inf\n","            best_coeff = None\n","            best_class = None\n","            matched = False\n","\n","            for c in range(model.C):\n","                coeff = ComplexCoefficients(d_order, 1, model.device)\n","                opt = optim.Adam(coeff.parameters(), lr=lr)\n","                x_c = model.x_mat.unsqueeze(0)\n","                p_c = model.p_mat.unsqueeze(0)\n","\n","                # Optimize coefficients for reconstruction\n","                for _ in range(epochs):\n","                    opt.zero_grad()\n","                    a = coeff(torch.tensor([0], device=model.device))\n","                    recon = model._reconstruct(a, x_c, p_c)\n","                    loss = (recon - H).abs().pow(2).sum()\n","                    loss.backward()\n","                    opt.step()\n","\n","                a_opt = coeff.all()\n","                recon = model._reconstruct(a_opt, x_c, p_c)\n","                rec_loss = (recon - H).abs().pow(2).sum().item()\n","                U_c = U[c]\n","                U_c_dag = U_c.conj().T\n","                transformed = U_c @ recon @ U_c_dag\n","                target_c = model.class_targets[c:c+1]\n","                tgt_diff = (transformed - target_c).abs().pow(2).sum().item()\n","\n","                if tgt_diff < tol_target:\n","                    matched = True\n","                    if rec_loss < best_match_loss:\n","                        best_match_loss = rec_loss\n","                        best_coeff, best_class = a_opt.squeeze(0), c\n","                elif not matched and rec_loss < best_nomatch_loss:\n","                    best_nomatch_loss = rec_loss\n","                    best_coeff, best_class = a_opt.squeeze(0), c\n","\n","            coeffs_out[idx] = best_coeff\n","            preds[idx] = best_class\n","\n","    print(\"Classification complete\")\n","    return coeffs_out, preds\n","\n","def run_pipeline(\n","    train_mats: np.ndarray,\n","    train_labels: np.ndarray,\n","    test_mats: np.ndarray,\n","    test_labels: np.ndarray,\n","    *,\n","    matrix_size: int = 64,\n","    d_order: int = 10,\n","    lr: float = 2e-3,\n","    epochs: int = 100,\n","    batch_size: int = 64,\n","    chunk_size: int = 100,\n","    margin: float = 1.0,\n",") -> Tuple[OvRMultiMarginHermitianClassifier, torch.Tensor, torch.Tensor, torch.Tensor]:\n","\n","    set_seed(42)\n","    tgt_train = create_target_matrices(train_labels, matrix_size)\n","    n_classes = int(train_labels.max() + 1)\n","\n","    model = OvRMultiMarginHermitianClassifier(\n","        matrix_size=matrix_size,\n","        n_classes=n_classes,\n","        d_order=d_order,\n","        lr=lr,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        chunk_size=chunk_size,\n","        margin=margin,\n","    )\n","    model.load_data(train_mats, train_labels, test_mats, test_labels, tgt_train)\n","    coeff_tr = train_model(model)\n","    #coeff_te, preds_te = classify_test_set(model, d_order, lr=lr*0.1)\n","\n","    print(f\"Train accuracy: {model.accuracy(model.Mtr, model.y):.2%}\")\n","    print(f\"Test accuracy:  {model.accuracy(model.Mte, model.yte):.2%}\")\n","    #print(f\"Test class-acc: {(preds_te.cpu().numpy() == test_labels).mean():.2%}\")\n","\n","\n","    return model, coeff_tr, coeff_te, preds_te\n","\n","def create_labels_from_class_counts(class_counts):\n","    labels = []\n","    for class_idx, count in enumerate(class_counts):\n","        labels.extend([class_idx] * count)\n","    return labels\n","\n","class_counts = [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n","\n","train_X = normalized_Hermitian_Digit_matrices.numpy()\n","train_y = np.array(create_labels_from_class_counts(class_counts))\n","test_X = normalized_hermitian_matrices_test_input.numpy()\n","test_y = np.array(y_test)\n","\n","run_pipeline(train_X, train_y, test_X, test_y, matrix_size = 64, d_order = 100, lr = 1e-1, epochs = 60, batch_size = 100)"],"metadata":{"id":"vY6FAaOZlQkB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Case 2 + Case 5:-**\n","\n","\n","This is an improved extension for class 2 where the coefficnets(xcap and pcap) as well as unitary matrix(U) are seperate for seperate classes."],"metadata":{"id":"mUOY4_lmmihw"}},{"cell_type":"code","source":["# x and p separate for separate classes , U seperate for seperate classes...(OVR Strategy) - Case 2 plus Case 5\n","# For d_order = 10 , perform sweeps d_order = 10, 100 ,1000 , and find the sweet point of best accuracy...\n","\n","# Accuracy for d_order 10 - 76 percent\n","\n","from __future__ import annotations\n","import time\n","from typing import Tuple, List\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","\n","def set_seed(seed: int = 42) -> None:\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","\n","\n","def create_target_matrices(labels: np.ndarray | List[int], n: int = 64) -> np.ndarray:\n","    \"\"\"Create diagonal target matrices with class-specific positions.\"\"\"\n","    pos = [0, 7, 14, 21, 28, 35, 42, 49, 56, 63]\n","    out = np.zeros((len(labels), n, n), np.complex64)\n","    for i, lab in enumerate(labels):\n","        out[i, pos[lab], pos[lab]] = 1.0\n","    norms = np.linalg.norm(out, axis=(-2, -1), ord='fro', keepdims=True)\n","    out = out / (norms + 1e-8)\n","    return out\n","\n","\n","class ComplexCoefficients(nn.Module):\n","    def __init__(self, d: int, n_samples: int, device: torch.device):\n","        super().__init__()\n","        std = 0.01 / np.sqrt(d)\n","        self.real = nn.Parameter(std * torch.randn(n_samples, d, device=device))\n","        self.imag = nn.Parameter(std * torch.randn(n_samples, d, device=device))\n","\n","    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n","        return torch.complex(self.real[idx], self.imag[idx])\n","\n","    def all(self) -> torch.Tensor:\n","        return torch.complex(self.real, self.imag)\n","\n","\n","class ReduceLROnPlateau:\n","    \"\"\"Simple learning rate scheduler.\"\"\"\n","    def __init__(self, opt: optim.Optimizer, factor=0.5, patience=3, min_lr=1e-8):\n","        self.opt, self.factor, self.patience, self.min_lr = opt, factor, patience, min_lr\n","        self.best, self.bad = None, 0\n","\n","    def step(self, metric: float):\n","        if self.best is None or metric < self.best:\n","            self.best, self.bad = metric, 0\n","            return\n","        self.bad += 1\n","        if self.bad >= self.patience:\n","            for pg in self.opt.param_groups:\n","                pg[\"lr\"] = max(pg[\"lr\"] * self.factor, self.min_lr)\n","            self.bad = 0\n","\n","\n","class OvRMultiMarginHermitianClassifier(nn.Module):\n","    \"\"\"\n","    Multi-class classifier with:\n","    - Class-specific Hermitian bases (x_c, p_c) for each class\n","    - Separate unitary matrices U_k for each class (OvR strategy)\n","    - Multi-margin hinge loss objective\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        matrix_size: int = 64,\n","        n_classes: int = 10,\n","        d_order: int = 10,\n","        lr: float = 2e-3,\n","        epochs: int = 100,\n","        batch_size: int = 128,\n","        chunk_size: int = 100,\n","        margin: float = 1.0,\n","        device: str | None = None,\n","    ) -> None:\n","        super().__init__()\n","\n","        self.n, self.C, self.d = matrix_size, n_classes, d_order\n","        self.lr, self.epochs, self.batch_size = lr, epochs, batch_size\n","        self.chunk_size = chunk_size\n","        self.margin = margin\n","        self.device = torch.device(device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n","\n","        # Class-wise Hermitian bases (x_c, p_c) → Parameters (C, n, n)\n","        self.x_cls = nn.Parameter(0.1 * torch.randn(self.C, self.n, self.n, dtype=torch.cfloat,device = self.device))\n","        self.p_cls = nn.Parameter(0.1 * torch.randn(self.C, self.n, self.n, dtype=torch.cfloat,device = self.device))\n","\n","        # Separate unitary parameters for each class (C, n, n)\n","        self.U_param = nn.Parameter(torch.randn(self.C, self.n, self.n, dtype=torch.cfloat,device = self.device))\n","\n","        # Class-specific target matrices (C, n, n)\n","        class_targets = create_target_matrices(list(range(self.C)), self.n)\n","        self.class_targets = torch.tensor(class_targets, dtype=torch.cfloat, device=self.device)\n","\n","        # Data placeholders\n","        self.Mtr = self.Mte = self.y = self.yte = self.target_mats = None\n","\n","        self.to(self.device)\n","\n","    @staticmethod\n","    def _make_hermitian(M: torch.Tensor) -> torch.Tensor:\n","        return 0.5 * (M + M.conj().transpose(-2, -1))\n","\n","    @staticmethod\n","    def _fro_norm(M: torch.Tensor) -> torch.Tensor:\n","        return torch.norm(M, p=\"fro\", dim=(-2, -1), keepdim=True) + 1e-8\n","\n","    def _normalise(self, M: torch.Tensor) -> torch.Tensor:\n","        return M / self._fro_norm(M)\n","\n","    def _make_unitary(self, M: torch.Tensor) -> torch.Tensor:\n","        H = self._make_hermitian(M)\n","        return torch.matrix_exp(-1j * H)\n","\n","    def _reconstruct(\n","        self,\n","        coeffs: torch.Tensor,      # (B, d)\n","        x_b: torch.Tensor,         # (B, n, n) - class-selected x matrices\n","        p_b: torch.Tensor,         # (B, n, n) - class-selected p matrices\n","    ) -> torch.Tensor:\n","        \"\"\"Vectorized reconstruction for a batch.\"\"\"\n","        B = coeffs.shape[0]\n","        x_h = self._make_hermitian(self._normalise(x_b))\n","        p_h = self._make_hermitian(self._normalise(p_b))\n","\n","        # Base Hamiltonian: H0 = 0.5*(p²) + 0.5*(x²)\n","        H0 = self._make_hermitian(0.5 * (p_h @ p_h) + 0.5 * (x_h @ x_h))\n","\n","        # Compute powers of x\n","        powers = []\n","        x_pow = x_h @ x_h\n","        powers.append(x_pow)\n","        for _ in range(1, self.d):\n","            x_pow = x_pow @ x_h\n","            powers.append(x_pow)\n","\n","        # Reconstruct: H = H0 + Σ a_k * x^{k+2}\n","        recon = H0.clone()\n","        for k in range(self.d):\n","            recon = recon + coeffs[:, k].view(B, 1, 1) * powers[k]\n","\n","        recon = 0.5 * (recon + recon.conj().transpose(-2, -1))\n","        return recon\n","\n","    def forward(\n","        self,\n","        coeffs: torch.Tensor,      # (B, d)\n","        labels: torch.Tensor,      # (B,)\n","        originals: torch.Tensor,   # (B, n, n)\n","    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n","\n","        B = originals.size(0)\n","\n","        # Get class-specific bases for this batch\n","        x_b = self.x_cls[labels]  # (B, n, n)\n","        p_b = self.p_cls[labels]  # (B, n, n)\n","\n","        # Reconstruct Hamiltonians\n","        recon = self._reconstruct(coeffs, x_b, p_b)\n","\n","        # 1) Reconstruction loss\n","        diff = recon - originals\n","        L_recon = torch.diagonal(diff.conj().transpose(-2, -1) @ diff,\n","                                dim1=-2, dim2=-1).sum(-1).abs().mean()\n","\n","        # 2) Multi-margin hinge loss with separate unitaries\n","        U = self._make_unitary(self.U_param)  # (C, n, n)\n","\n","        # Compute scores for all classes (memory-efficient)\n","        scores = torch.zeros(B, self.C, device=self.device)\n","        for k in range(self.C):\n","            U_k = U[k]  # (n, n)\n","            U_k_dag = U_k.conj().T\n","            target_k = self.class_targets[k]\n","            transformed_k = U_k @ recon @ U_k_dag\n","            # Score = negative distance to target_k (higher is better)\n","            dists_k = torch.norm(transformed_k - target_k.unsqueeze(0), p=\"fro\", dim=(-2, -1))\n","            scores[:, k] = -dists_k\n","\n","        # Multi-margin hinge loss: L = (1/C) * Σ_{j≠i} max(0, margin + s_j - s_i)\n","        correct_scores = scores[torch.arange(B), labels]\n","\n","        hinge_losses = torch.zeros(B, device=self.device)\n","        for i in range(B):\n","            correct_score = correct_scores[i]\n","            wrong_scores = torch.cat([scores[i, :labels[i]], scores[i, labels[i]+1:]])\n","            margins = torch.clamp(self.margin + wrong_scores - correct_score, min=0.0)\n","            hinge_losses[i] = margins.mean()\n","\n","        L_multimargin = hinge_losses.mean()\n","\n","        # 3) Unitary constraints (batched)\n","        I = torch.eye(self.n, dtype=torch.cfloat, device=self.device)\n","        U_dag = U.conj().transpose(-2, -1)\n","        L_uc = torch.norm(U @ U_dag - I, p=\"fro\") + torch.norm(U_dag @ U - I, p=\"fro\")\n","\n","        # Total loss\n","        total = L_recon + 5.0 * L_multimargin + 0.1 * L_uc\n","        return total, L_recon, L_multimargin, L_uc\n","\n","    @torch.no_grad()\n","    def accuracy(self, mats: torch.Tensor, labels: torch.Tensor) -> float:\n","        \"\"\"Memory-efficient accuracy computation using chunking.\"\"\"\n","        U = self._make_unitary(self.U_param)  # (C, n, n)\n","\n","        total_correct = 0\n","        total_samples = 0\n","\n","        # Process in chunks to avoid memory issues\n","        for start_idx in range(0, mats.size(0), self.chunk_size):\n","            end_idx = min(start_idx + self.chunk_size, mats.size(0))\n","            chunk_mats = mats[start_idx:end_idx]\n","            chunk_labels = labels[start_idx:end_idx]\n","\n","            chunk_size_actual = chunk_mats.size(0)\n","            scores = torch.zeros(chunk_size_actual, self.C, device=self.device)\n","\n","            # Compute scores for each class\n","            for k in range(self.C):\n","                U_k = U[k]\n","                U_k_dag = U_k.conj().T\n","                target_k = self.class_targets[k]\n","\n","                transformed_k = U_k @ chunk_mats @ U_k_dag\n","                dists_k = torch.norm(transformed_k - target_k.unsqueeze(0), p=\"fro\", dim=(-2, -1))\n","                scores[:, k] = -dists_k  # higher is better\n","\n","            preds = torch.argmax(scores, dim=1)\n","            total_correct += (preds == chunk_labels).sum().item()\n","            total_samples += chunk_size_actual\n","\n","        return total_correct / total_samples\n","\n","    def load_data(\n","        self,\n","        train_mats: np.ndarray,\n","        train_labels: np.ndarray,\n","        test_mats: np.ndarray,\n","        test_labels: np.ndarray,\n","        target_mats: np.ndarray,\n","    ) -> None:\n","        print(f\"Loading data on {self.device} …\")\n","\n","        self.Mtr = torch.tensor(train_mats, dtype=torch.cfloat, device=self.device)\n","        self.y   = torch.tensor(train_labels, dtype=torch.long, device=self.device)\n","        self.Mte = torch.tensor(test_mats, dtype=torch.cfloat, device=self.device)\n","        self.yte = torch.tensor(test_labels, dtype=torch.long, device=self.device)\n","        self.target_mats = torch.tensor(target_mats, dtype=torch.cfloat, device=self.device)\n","\n","        print(f\"Data loaded — train {self.Mtr.shape}, test {self.Mte.shape}\")\n","\n","\n","def train_model(model: OvRMultiMarginHermitianClassifier) -> torch.Tensor:\n","    \"\"\"Training routine with multi-margin hinge loss.\"\"\"\n","    print(\"► Training started …\")\n","    t0 = time.time()\n","\n","    N = model.Mtr.shape[0]\n","    coeffs = ComplexCoefficients(model.d, N, model.device)\n","\n","    # Optimizers with different learning rates\n","    opt_coeff = optim.Adam(coeffs.parameters(), lr=model.lr)\n","    opt_bases = optim.Adam([model.x_cls, model.p_cls], lr=model.lr * 0.1)\n","    opt_units = optim.Adam([model.U_param], lr=model.lr * 0.05)\n","\n","    # Schedulers\n","    sch_coeff = ReduceLROnPlateau(opt_coeff)\n","    sch_bases = ReduceLROnPlateau(opt_bases)\n","    sch_units = ReduceLROnPlateau(opt_units)\n","\n","    # DataLoader\n","    ds = TensorDataset(model.Mtr, model.target_mats, model.y, torch.arange(N, device=model.device))\n","    dl = DataLoader(ds, batch_size=model.batch_size, shuffle=True)\n","\n","    for ep in range(1, model.epochs + 1):\n","        print(f\"Epoch : {ep}\")\n","        tot_sum = rec_sum = margin_sum = 0.0\n","        batches = 0\n","\n","        for H, T, lab, idx in dl:\n","            H, T, lab, idx = H.to(model.device), T.to(model.device), lab.to(model.device), idx.to(model.device)\n","            a = coeffs(idx)\n","\n","            # Zero gradients\n","            opt_coeff.zero_grad()\n","            opt_bases.zero_grad()\n","            opt_units.zero_grad()\n","\n","            # Forward pass\n","            tot, L_recon, L_margin, L_uc = model(a, lab, H)\n","            tot.backward()\n","\n","            # Gradient clipping\n","            torch.nn.utils.clip_grad_norm_(coeffs.parameters(), 1.0)\n","            torch.nn.utils.clip_grad_norm_([model.x_cls, model.p_cls], 0.5)\n","            torch.nn.utils.clip_grad_norm_([model.U_param], 0.5)\n","\n","            # Update parameters\n","            opt_coeff.step()\n","            opt_bases.step()\n","            opt_units.step()\n","\n","            tot_sum += tot.item()\n","            rec_sum += L_recon.item()\n","            margin_sum += L_margin.item()\n","            batches += 1\n","            del H, T, lab, idx\n","            torch.cuda.empty_cache()\n","\n","\n","        sch_coeff.step(tot_sum / batches)\n","        sch_bases.step(rec_sum / batches)\n","        sch_units.step(margin_sum / batches)\n","\n","        if ep % 1 == 0 or ep == 1:\n","            train_acc = model.accuracy(model.Mtr, model.y)\n","            test_acc = model.accuracy(model.Mte, model.yte)\n","            acc_str = f\"Acc train {train_acc:.3f} │ test {test_acc:.3f} | \"\n","        else:\n","            acc_str = \"\"\n","\n","        print(f\"  Ep {ep:03d}/{model.epochs} | \"\n","              f\"Loss {tot_sum/batches:.4e} | \"\n","              f\"Recon {rec_sum/batches:.4e} | \"\n","              f\"Margin {margin_sum/batches:.4e} | \"\n","              f\"{acc_str}\"\n","              f\"Δt {time.time()-t0:.1f}s\")\n","\n","    print(f\"✓ Training finished in {time.time() - t0:.1f}s\")\n","    xcap_np = model.x_cls.detach().cpu().numpy()\n","    pcap_np = model.p_cls.detach().cpu().numpy()\n","    return xcap_np, pcap_np, coeffs.all()\n","\n","\n","def classify_test_set(\n","    model: OvRMultiMarginHermitianClassifier,\n","    d_order: int,\n","    lr: float = 1e-3,\n","    epochs: int = 100,\n","    batch_size: int = 1000,\n","    tol_target: float = 1e-3,\n",") -> Tuple[torch.Tensor, torch.Tensor]:\n","    \"\"\"\n","    Classify test samples by finding best reconstruction for each class.\n","\n","    Returns:\n","        coeffs_out: (N_test, d) coefficients of the chosen class per sample\n","        preds: (N_test,) predicted class labels\n","    \"\"\"\n","    print(\"► Classifying test set …\")\n","    N = model.Mte.shape[0]\n","    coeffs_out = torch.empty((N, d_order), dtype=torch.cfloat, device=model.device)\n","    preds = torch.empty(N, dtype=torch.long, device=model.device)\n","\n","    # Pre-compute unitaries and targets\n","    U = model._make_unitary(model.U_param)  # (C, n, n)\n","\n","    ds = TensorDataset(model.Mte, torch.arange(N, device=model.device))\n","    #ds = TensorDataset(torch.tensor(test_mat[:1000],dtype=torch.cfloat, device=model.device), torch.arange(N, device=model.device))\n","    dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n","    i = 0\n","    while i == 0:\n","        for H_batch, idx_batch in dl:\n","            while i == 0:\n","                H_batch = H_batch.to(model.device)\n","                idx_batch = idx_batch.to(model.device)\n","                print(f\"Idx batch: {idx_batch}\")\n","                for j, idx in enumerate(idx_batch):\n","                    print(f\"{j},{idx}\")\n","                    H = H_batch[j:j+1]  # (1, n, n)\n","\n","                    best_match_loss = torch.inf\n","                    best_nomatch_loss = torch.inf\n","                    best_coeff = None\n","                    best_class = None\n","                    matched = False\n","\n","                    for c in range(model.C):\n","                        coeff = ComplexCoefficients(d_order, 1, model.device)\n","                        opt = optim.Adam(coeff.parameters(), lr=lr)\n","\n","                        # Class-specific bases\n","                        x_c = model.x_cls[c:c+1]  # (1, n, n)\n","                        p_c = model.p_cls[c:c+1]  # (1, n, n)\n","\n","                        # Optimize coefficients for reconstruction\n","                        for _ in range(epochs):\n","                            opt.zero_grad()\n","                            a = coeff(torch.tensor([0], device=model.device))\n","                            recon = model._reconstruct(a, x_c, p_c)\n","                            loss = (recon - H).abs().pow(2).sum()\n","                            loss.backward()\n","                            opt.step()\n","\n","                        # Get final coefficients and reconstruction\n","                        a_opt = coeff.all()  # (1, d)\n","                        recon = model._reconstruct(a_opt, x_c, p_c)\n","                        rec_loss = (recon - H).abs().pow(2).sum().item()\n","\n","                        # Check if transformation matches class target\n","                        U_c = U[c]\n","                        U_c_dag = U_c.conj().T\n","                        transformed = U_c @ recon @ U_c_dag\n","                        target_c = model.class_targets[c:c+1]\n","                        tgt_diff = (transformed - target_c).abs().pow(2).sum().item()\n","\n","                        # Classification logic\n","                        if tgt_diff < tol_target:  # Matches class target\n","                            matched = True\n","                            if rec_loss < best_match_loss:\n","                                best_match_loss = rec_loss\n","                                best_coeff, best_class = a_opt.squeeze(0), c\n","                        elif not matched and rec_loss < best_nomatch_loss:\n","                            best_nomatch_loss = rec_loss\n","                            best_coeff, best_class = a_opt.squeeze(0), c\n","\n","                    coeffs_out[idx] = best_coeff\n","                    preds[idx] = best_class\n","                    i = 1\n","\n","\n","    print(\"Classification complete\")\n","    return coeffs_out, preds\n","\n","\n","def run_pipeline(\n","    train_mats: np.ndarray,\n","    train_labels: np.ndarray,\n","    test_mats: np.ndarray,\n","    test_labels: np.ndarray,\n","    *,\n","    matrix_size: int = 64,\n","    d_order: int = 10,\n","    lr: float = 2e-3,\n","    epochs: int = 100,\n","    batch_size: int = 64,\n","    chunk_size: int = 100,\n","    margin: float = 1.0,\n",") -> Tuple[OvRMultiMarginHermitianClassifier, torch.Tensor, torch.Tensor, torch.Tensor]:\n","\n","    set_seed(42)\n","\n","    # Create target matrices for training\n","    tgt_train = create_target_matrices(train_labels, matrix_size)\n","    n_classes = int(train_labels.max() + 1)\n","\n","    # Initialize model\n","    model = OvRMultiMarginHermitianClassifier(\n","        matrix_size=matrix_size,\n","        n_classes=n_classes,\n","        d_order=d_order,\n","        lr=lr,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        chunk_size=chunk_size,\n","        margin=margin,\n","    )\n","    model.load_data(train_mats, train_labels, test_mats, test_labels, tgt_train)\n","\n","    # Train the model\n","    x,p,coeff_tr = train_model(model)\n","\n","    # Classify test set\n","    coeff_te, preds_te = classify_test_set(model, d_order,lr=lr*0.1,batch_size = 15)\n","    print(coeff_te.shape)\n","    # Final results\n","    print(f\"Train accuracy: {model.accuracy(model.Mtr, model.y):.2%}\")\n","    print(f\"Test accuracy:  {model.accuracy(model.Mte, model.yte):.2%}\")\n","    #print(f\"Test class-acc: {(preds_te.cpu().numpy() == test_labels[:5]).mean():.2%}\")\n","    print(\"===============================================\\n\")\n","\n","    return model, coeff_te.cpu(), preds_te.cpu(), x, p, coeff_tr\n","\n","\n","\n","# Perform d_order sweeps 10,100,1000 , and find the sweet point by Grid Search Sweep ...\n","\n","def create_labels_from_class_counts(class_counts):\n","    labels = []\n","    for class_idx, count in enumerate(class_counts):\n","        labels.extend([class_idx] * count)\n","    return labels\n","\n","class_counts = [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n","\n","train_X = normalized_Hermitian_Digit_matrices.numpy()\n","train_y = np.array(create_labels_from_class_counts(class_counts))\n","test_X = normalized_hermitian_matrices_test_input.numpy()\n","test_y = np.array(y_test)\n","\n","model,coeff_te, preds_te, x, p, coeff_tr = run_pipeline(train_X, train_y, test_X, test_y, matrix_size = 64, d_order = 20, lr = 1e-1, epochs = 60, batch_size = 100)"],"metadata":{"id":"PG8I5ggwlMMT"},"execution_count":null,"outputs":[]}]}