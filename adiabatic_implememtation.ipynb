{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27VYQVb-a3HE"
   },
   "source": [
    "In this approach, we implement a quantum-inspired classification method for MNIST dataset by representing classical images as quantum Hamiltonians and then employing the principle of Adiabatic Thereom to find a quantum state that corrosponds to specific image, and then perform classification on the basis of its similarity to other states.\n",
    "\n",
    "We have used 2 methods for the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3UuDHFbcNnC"
   },
   "source": [
    "Data preparation:\n",
    "\n",
    "We use the MNIST dataset consisting of 60000 training images and 1000 tet images.The original size of the images is (784,). We reshape the images to size (28,28), normalize the images and then convert the images into hamiltonians using 4 methods\n",
    "\n",
    "1) H = (A + A.T) / 2\n",
    "\n",
    "2)H = AA.T\n",
    "\n",
    "3)H = outer product of flattened image vectors\n",
    "\n",
    "4) H = -i*log(V) where V is a unitary matrix.\n",
    "\n",
    "Then all the hamiltonians are seperated to 10 classes on the basis of the digits from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1uSs9F6cUUi"
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing of the MNIST Dataset to produce the train and test normalized Hamiitonians...\n",
    "# We can construct the hamiltonians from the four methods described in the paper...\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from sklearn.datasets import fetch_openml\n",
    "import scipy\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load MNIST using Keras\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# # Reshape and convert to float64 for consistency\n",
    "x_train = x_train.reshape(-1, 784).astype(np.float64)\n",
    "x_test = x_test.reshape(-1, 784).astype(np.float64)\n",
    "print(\"Train:\", x_train.shape, y_train.shape)\n",
    "print(\"Test:\", x_test.shape, y_test.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# Helper functions\n",
    "# ----------------------------\n",
    "def separate_digits(images, labels):\n",
    "    \"\"\"Group images by digit label.\"\"\"\n",
    "    digit_image = {d: [] for d in range(10)}\n",
    "    for img, lbl in zip(images, labels):\n",
    "        digit_image[lbl].append(img)\n",
    "    return digit_image\n",
    "\n",
    "def resize_images_batch(images, new_size=(8, 8), batch_size=500):\n",
    "    \"\"\"Resize a batch of flattened 28x28 images to new_size.\"\"\"\n",
    "    n = len(images)\n",
    "    resized = []\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = images[i:i+batch_size]\n",
    "        resized_batch = [resize(img.reshape(28,28), new_size).flatten() for img in batch]\n",
    "        resized.extend(resized_batch)\n",
    "    return np.array(resized)\n",
    "\n",
    "def normalize_batch(images):\n",
    "    \"\"\"Normalize each image vector.\"\"\"\n",
    "    norms = np.linalg.norm(images, axis=1, keepdims=True)\n",
    "    return images / norms\n",
    "\n",
    "#Creating Hamiltonian using outer product method\n",
    "def density_matrix_batch(images):\n",
    "    \"\"\"Convert vectors to density matrices.\"\"\"\n",
    "    return np.matmul(images[:,:,np.newaxis], images[:,np.newaxis,:])\n",
    "\n",
    "#Creating the Hamiltonian using H = A + A.T/2 method\n",
    "def hamiltonian_symmetric_batch(images):\n",
    "  N,D = images.shape\n",
    "  H_list = []\n",
    "  for i in range(N):\n",
    "    a = images[i]\n",
    "    A = np.outer(a,np.ones(D))\n",
    "    H = (A + A.conj().T) / 2\n",
    "    H_list.append(H)\n",
    "\n",
    "  return np.array(H_list)\n",
    "\n",
    "#Creating the Hamiltonian using H = A @ A.T method\n",
    "def hamiltonian_product_batch(images):\n",
    "  N , D = images.shape\n",
    "  H_list = []\n",
    "  for i in range(N):\n",
    "    a = images[i]\n",
    "    A = np.outer(a,np.ones(D))\n",
    "    H = A @ A.T\n",
    "    H_list.append(H)\n",
    "  return np.array(H_list)\n",
    "\n",
    "import scipy.linalg\n",
    "#Creating the Hamiltonian using H = -i * log(V) method\n",
    "def hamiltonian_using_log(images):\n",
    "    def _make_hermitian(M):\n",
    "        return 0.5 * (M + M.conj().transpose(-2,-1))\n",
    "\n",
    "    def _make_unitary(M):\n",
    "        H = _make_hermitian(M)\n",
    "        return torch.matrix_exp(-1j*H)\n",
    "\n",
    "    N,D = images.shape\n",
    "    hamiltonians = np.zeros((N,D,D),dtype = np.complex128)\n",
    "    for i in range(N):\n",
    "        image = images[i]\n",
    "        mat = np.diag(image)\n",
    "        mat_torch = torch.tensor(mat,dtype = torch.complex128)\n",
    "        H = _make_unitary(mat_torch)\n",
    "        hamiltonians[i] = H\n",
    "    return hamiltonians\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Process training data\n",
    "# ----------------------------\n",
    "digit_images_dict = separate_digits(x_train, y_train)\n",
    "resized_digit_images = {}\n",
    "normalized_digit_images = {}\n",
    "density_matrices = {}\n",
    "\n",
    "for digit, imgs in digit_images_dict.items():\n",
    "    imgs = np.array(imgs)\n",
    "    imgs_resized = resize_images_batch(imgs, new_size=(8,8), batch_size=500)\n",
    "    imgs_normalized = normalize_batch(imgs_resized)\n",
    "    print(f\"normalized_images shape:- {imgs_normalized.shape}\")\n",
    "    density1 = density_matrix_batch(imgs_normalized)\n",
    "    print(f\"shape 1:- {density1.shape}\")\n",
    "    #OR\n",
    "    #density2 = hamiltonian_symmetric_batch(imgs_normalized)\n",
    "    #print(f\"shape 2:- {density2.shape}\")\n",
    "    #OR\n",
    "    #density3 = hamiltonian_product_batch(imgs_normalized)\n",
    "    #print(f\"shape 3:- {density3.shape}\")\n",
    "    #OR\n",
    "    #density4 = hamiltonian_using_log(imgs_normalized)\n",
    "    #print(f\"shape 4:- {density4.shape}\")\n",
    "    density1 /= np.linalg.norm(density1, axis=(1,2), keepdims=True)\n",
    "    #density2 /= np.linalg.norm(density2, axis=(1,2), keepdims=True)\n",
    "    #density3 /= np.linalg.norm(density3, axis=(1,2), keepdims=True)\n",
    "    #density4 /= np.linalg.norm(density4, axis=(1,2), keepdims=True)\n",
    "    resized_digit_images[digit] = imgs_resized\n",
    "    normalized_digit_images[digit] = imgs_normalized\n",
    "    density_matrices[digit] = density1\n",
    "    #density_matrices[digit] = density2\n",
    "    #density_matrices[digit] = density3\n",
    "    #density_matrices[digit] = density4\n",
    "\n",
    "train_density_matrices = np.concatenate([density_matrices[d] for d in range(10)], axis=0)\n",
    "train_density_matrices_tensor = torch.tensor(train_density_matrices, dtype=torch.cfloat)\n",
    "\n",
    "# ----------------------------\n",
    "# Process test data\n",
    "# ----------------------------\n",
    "test_images_resized = np.array([resize(img.reshape(28,28), (8,8)).flatten() for img in x_test])\n",
    "test_normed = normalize_batch(test_images_resized)\n",
    "test_density = density_matrix_batch(test_normed)\n",
    "test_density /= np.linalg.norm(test_density, axis=(1,2), keepdims=True)\n",
    "test_density_tensor = torch.tensor(test_density, dtype=torch.cfloat)\n",
    "\n",
    "# ----------------------------\n",
    "# Visualization example\n",
    "# ---------------------------\n",
    "\n",
    "for digit in range(10):\n",
    "    images_to_plot = resized_digit_images[digit][:10]\n",
    "    plt.figure(figsize=(10,2))\n",
    "    for i in range(10):\n",
    "        plt.subplot(1, 10, i+1)\n",
    "        plt.imshow(images_to_plot[i].reshape(8,8), cmap='magma')\n",
    "        plt.title(f\"{digit}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "normalized_Hermitian_Digit_matrices = train_density_matrices_tensor\n",
    "normalized_hermitian_matrices_test_input = test_density_tensor\n",
    "\n",
    "print(f\"normalized_Hermitian_Digit_matrices shape:- {normalized_Hermitian_Digit_matrices.shape}\")\n",
    "print(f\"normalized_hermitian_matrices_test_input shape:- {normalized_hermitian_matrices_test_input.shape}\")\n",
    "\n",
    "labels = []\n",
    "for i in range(10):\n",
    "    labels.append(i)\n",
    "\n",
    "print(labels)\n",
    "\n",
    "D = [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n",
    "# D = [100] * 10\n",
    "labels_zero = [labels[0]]*D[0]\n",
    "labels_one  = [labels[1]]*D[1]\n",
    "labels_two  = [labels[2]]*D[2]\n",
    "labels_three  = [labels[3]]*D[3]\n",
    "labels_four  = [labels[4]]*D[4]\n",
    "labels_five  = [labels[5]]*D[5]\n",
    "labels_six  = [labels[6]]*D[6]\n",
    "labels_seven  = [labels[7]]*D[7]\n",
    "labels_eigth  = [labels[8]]*D[8]\n",
    "labels_nineth  = [labels[9]]*D[9]\n",
    "labels_zero = np.array(labels_zero,dtype = int)\n",
    "labels_one = np.array(labels_one,dtype = int)\n",
    "labels_two = np.array(labels_two,dtype = int)\n",
    "labels_three = np.array(labels_three,dtype = int)\n",
    "labels_four = np.array(labels_four,dtype = int)\n",
    "labels_five = np.array(labels_five,dtype = int)\n",
    "labels_six = np.array(labels_six,dtype = int)\n",
    "labels_seven = np.array(labels_seven,dtype = int)\n",
    "labels_eigth = np.array(labels_eigth,dtype = int)\n",
    "labels_nineth = np.array(labels_nineth,dtype = int)\n",
    "\n",
    "labels_new_train = np.concatenate((labels_zero,labels_one))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_two))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_three))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_four))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_five))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_six))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_seven))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_eigth))\n",
    "labels_new_train = np.concatenate((labels_new_train,labels_nineth))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHBMy0VXbb8I"
   },
   "source": [
    "**Method 1:-**\n",
    "\n",
    "In this method, the classical data(MNIST images) are converted into quantum Hamiltonians by representing each image as a matrix.A neural network is then used to train the 10 class Hamiltonians one for each digit, which act as prototypes.In classification phase, a new images is first converted into quantum Hamiltonian and this Hamiltonian is compared to all the mean trained Hamiltonians of the 10 classes using Frobenius norm as the distance metric. The images is classified as the digit whose mean class Hamiltonian is closest or has smallest Frobenius norm.\n",
    "\n",
    "Here to obtain the training hamiltonians we have made use of 3 unitary methods\n",
    "\n",
    "1) Using Singular Value decomposition(make_unitary1)\n",
    "\n",
    "2)Using QR decomposition(make_unitary2)\n",
    "\n",
    "3)Using Quantum unitary encoding.(make_unitary3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxDL67B-OCcj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#Class to handle the training hamiltonian matrices\n",
    "class MatrixModel(nn.Module):\n",
    "  def __init__(self,num_classes = 10, matrix_size = 64):\n",
    "    super().__init__()\n",
    "    self.num_classes = num_classes\n",
    "    self.matrix_size = matrix_size\n",
    "    self.eigenvalues = nn.Parameter(torch.randn(num_classes,matrix_size,dtype = torch.float64))\n",
    "    self.eigenvectors_real = nn.Parameter(torch.randn(num_classes,matrix_size,matrix_size,dtype = torch.float64))\n",
    "    self.eigenvectors_imag = nn.Parameter(torch.randn(num_classes,matrix_size,matrix_size,dtype = torch.float64))\n",
    "\n",
    "  def get_complex_eigenvectors(self):\n",
    "    return torch.complex(self.eigenvectors_real, self.eigenvectors_imag)\n",
    "\n",
    "  def find_unitary_transformation(self,input_density_matrix, output_density_matrix):\n",
    "    X = np.dot(output_density_matrix, np.linalg.pinv(input_density_matrix))\n",
    "    U, S, V_dagger = np.linalg.svd(X, full_matrices=False)\n",
    "    phase_matrix = np.diag(np.exp(1j * np.angle(S)))\n",
    "    unitary_matrix = U @ (phase_matrix @ V_dagger)\n",
    "    unitary_matrix /= np.linalg.det(unitary_matrix)**(1/2)\n",
    "    return unitary_matrix\n",
    "\n",
    "  def is_unitary(matrix):\n",
    "    # Check if the matrix is unitary\n",
    "    identity = np.eye(matrix.shape[0])\n",
    "    return np.allclose(matrix @ matrix.conj().T, identity) and np.allclose(matrix.conj().T @ matrix, identity)\n",
    "\n",
    "\n",
    "\n",
    "  def make_unitary1(self,matrix):\n",
    "    U,_,Vh = torch.linalg.svd(matrix,full_matrices = False)\n",
    "    return U @ Vh\n",
    "\n",
    "  def make_unitary2(self,matrix):\n",
    "    matrix = matrix / (torch.linalg.norm(matrix) + 1e-12) #for -i log(V) case only\n",
    "    Q,R = torch.linalg.qr(matrix)\n",
    "    return Q\n",
    "\n",
    "  def make_unitary3(self,matrix):\n",
    "    # Lower triangular density matrix\n",
    "    matrix = matrix.detach().cpu().numpy()\n",
    "    input_density_matrix = np.zeros((64, 64), dtype=np.complex128)\n",
    "    input_density_matrix[0, 0] = 1.0\n",
    "    lower_triangular = np.tril(matrix)\n",
    "    lower_triangular_conj = np.conj(lower_triangular).T\n",
    "    Density_Matrix_Classical_Lower = lower_triangular + lower_triangular_conj\n",
    "\n",
    "\n",
    "    # Upper triangular density matrix\n",
    "    upper_triangular = np.triu(matrix)\n",
    "    upper_triangular_conj = np.conj(upper_triangular).T\n",
    "    Density_Matrix_Classical_Upper = upper_triangular + upper_triangular_conj\n",
    "\n",
    "    # Halve diagonal elements\n",
    "    np.fill_diagonal(Density_Matrix_Classical_Lower,\n",
    "                     Density_Matrix_Classical_Lower.diagonal() / 2)\n",
    "    np.fill_diagonal(Density_Matrix_Classical_Upper,\n",
    "                     Density_Matrix_Classical_Upper.diagonal() / 2)\n",
    "\n",
    "    # Normalize by trace\n",
    "    Density_Matrix_Classical_Lower_Normalized = (\n",
    "        Density_Matrix_Classical_Lower / np.trace(Density_Matrix_Classical_Lower)\n",
    "    )\n",
    "    Density_Matrix_Classical_Upper_Normalized = (\n",
    "        Density_Matrix_Classical_Upper / np.trace(Density_Matrix_Classical_Upper)\n",
    "    )\n",
    "\n",
    "    # Find unitary transformations\n",
    "    unitary_transformation_lower = self.find_unitary_transformation(\n",
    "        input_density_matrix, Density_Matrix_Classical_Lower_Normalized\n",
    "    )\n",
    "    unitary_transformation_upper = self.find_unitary_transformation(\n",
    "        input_density_matrix, Density_Matrix_Classical_Upper_Normalized\n",
    "    )\n",
    "\n",
    "    # Choose one â€” here we return the lower version\n",
    "    # return unitary_transformation_lower\n",
    "    return torch.from_numpy(unitary_transformation_lower).to(torch.complex128)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def get_hamiltonians_orig1(self):\n",
    "        eigenvectors_complex = self.get_complex_eigenvectors()\n",
    "        unitary_vecs = torch.stack([self.make_unitary1(mat) for mat in eigenvectors_complex])\n",
    "        diag_matrices = torch.diag_embed(self.eigenvalues.to(torch.complex128))\n",
    "        hamiltonians = unitary_vecs @ diag_matrices @ unitary_vecs.conj().transpose(-1, -2)\n",
    "        hamiltonians = (hamiltonians + hamiltonians.conj().transpose(-1, -2)) / 2\n",
    "        return hamiltonians\n",
    "\n",
    "  def get_hamiltonians_orig2(self):\n",
    "        eigenvectors_complex = self.get_complex_eigenvectors()\n",
    "        unitary_vecs = torch.stack([self.make_unitary2(mat) for mat in eigenvectors_complex])\n",
    "        diag_matrices = torch.diag_embed(self.eigenvalues.to(torch.complex128))\n",
    "        hamiltonians = unitary_vecs @ diag_matrices @ unitary_vecs.conj().transpose(-1, -2)\n",
    "        hamiltonians = (hamiltonians + hamiltonians.conj().transpose(-1, -2)) / 2\n",
    "        return hamiltonians\n",
    "\n",
    "  def get_hamiltonians_orig3(self):\n",
    "        eigenvectors_complex = self.get_complex_eigenvectors()\n",
    "        unitary_vecs = torch.stack([self.make_unitary3(mat) for mat in eigenvectors_complex])\n",
    "        diag_matrices = torch.diag_embed(self.eigenvalues.to(torch.complex128))\n",
    "        hamiltonians = unitary_vecs @ diag_matrices @ unitary_vecs.conj().transpose(-1, -2)\n",
    "        hamiltonians = (hamiltonians + hamiltonians.conj().transpose(-1, -2)) / 2\n",
    "\n",
    "        return hamiltonians\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self):\n",
    "    return self.get_hamiltonians_orig1() #OR get_hamiltonians_orig2,get_hamiltonians_orig3\n",
    "\n",
    "def combined_loss_batched(output, target_batch, labels_batch):\n",
    "  batch_size = target_batch.size(0)\n",
    "  class_hamiltonians = output[labels_batch]\n",
    "  # print(class_hamiltonians.shape)\n",
    "  # print(target_batch.shape)\n",
    "  # diff = torch.nan_to_num(class_hamiltonians - target_batch,nan = 0.0,posinf = 1e6,neginf = -1e6)\n",
    "  # losses = torch.linalg.norm(diff,dim = (1,2))\n",
    "  losses = torch.linalg.norm(class_hamiltonians - target_batch,dim = (1,2))\n",
    "  return torch.mean(losses)\n",
    "\n",
    "def combined_loss_batched2(output, target_batch, labels_batch):\n",
    "    batch_size = target_batch.size(0)\n",
    "\n",
    "    # Differentiable class selection\n",
    "    class_hamiltonians = torch.gather(\n",
    "        output, 1, labels_batch.view(-1, 1, 1, 1).expand(-1, 1, 64, 64)\n",
    "    ).squeeze(1)\n",
    "\n",
    "    # Convert target to Hamiltonian form\n",
    "    target_batch = target_batch.reshape(batch_size, -1)  # (n, 64)\n",
    "    target_batch = torch.einsum('bi,bj->bij', target_batch, target_batch.conj())  # (n, 64, 64)\n",
    "\n",
    "    losses = torch.linalg.norm(class_hamiltonians - target_batch, dim=(1, 2))\n",
    "    return torch.mean(losses)\n",
    "\n",
    "\n",
    "def create_labels_from_class_counts(class_counts):\n",
    "    labels = []\n",
    "    for class_idx, count in enumerate(class_counts):\n",
    "        labels.extend([class_idx] * count)\n",
    "    return labels\n",
    "\n",
    "def create_batched_data(data, labels, batch_size=64):\n",
    "    if not isinstance(data, torch.Tensor):\n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            data = torch.stack(data)\n",
    "        else:\n",
    "            data = torch.tensor(data)\n",
    "    if not isinstance(labels, torch.Tensor):\n",
    "        labels = torch.tensor(labels)\n",
    "    if not data.dtype == torch.complex128:\n",
    "        data = data.to(torch.complex128)\n",
    "    dataset = TensorDataset(data, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def train_model(model, dataloader,optimizer, scheduler, threshold = 0.0000001, num_epochs=100):\n",
    "    model.train()\n",
    "    all_losses = []\n",
    "    wait = 0\n",
    "    patience = 5\n",
    "    to_stop = 0\n",
    "    epoch = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch:- {epoch}\")\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch_data, batch_labels in dataloader:\n",
    "            if num_batches % 1000 == 0:\n",
    "              print(num_batches)\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model()\n",
    "            loss = combined_loss_batched(outputs, batch_data, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        scheduler.step(avg_loss)\n",
    "        diff = 0\n",
    "        if epoch > 2:\n",
    "          diff = all_losses[-1] - avg_loss\n",
    "        all_losses.append(avg_loss)\n",
    "        if epoch > 2 and avg_loss - all_losses[-1] < threshold:\n",
    "          print(\"less than threshold\")\n",
    "          if wait < patience:\n",
    "            wait = wait + 1\n",
    "          else:\n",
    "            to_stop = 1\n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Average Loss: {avg_loss:.4e}, Difference = {diff:.10e}')\n",
    "        epoch = epoch + 1\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "def inference(model, test_data, test_labels=None):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hamiltonians = model().cpu() # 10,64,64\n",
    "        # print(hamiltonians.shape)\n",
    "        predicted_labels = []\n",
    "\n",
    "        for test_sample in test_data:\n",
    "            if isinstance(test_sample, torch.Tensor):\n",
    "                test_sample = test_sample.cpu()\n",
    "            frobenius_norms = []\n",
    "            for class_idx in range(10):\n",
    "                # print(test_sample.shape)\n",
    "                # print(hamiltonians[class_idx].shape)\n",
    "                norm = torch.linalg.norm(test_sample - hamiltonians[class_idx], ord='fro')\n",
    "                frobenius_norms.append(norm.item())\n",
    "            predicted_labels.append(np.argmin(frobenius_norms))\n",
    "        if test_labels is not None:\n",
    "            accuracy = np.sum(np.array(predicted_labels) == np.array(test_labels))\n",
    "            accuracy_percent = (accuracy / len(test_labels)) * 100\n",
    "            return predicted_labels, accuracy_percent\n",
    "\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIvFXJL2edf-"
   },
   "source": [
    "We perform training using a batch size of 500 and number of epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSaWrnFMeQlt"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Creating the training data and labels\")\n",
    "class_counts = [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n",
    "training_labels = create_labels_from_class_counts(class_counts)\n",
    "training_data = torch.as_tensor(normalized_Hermitian_Digit_matrices,dtype = torch.complex128,device = device)\n",
    "print(f\"training data shape:- {training_data.shape}\")\n",
    "\n",
    "\n",
    "print(\"Initialising the model,optimier,scheduler\")\n",
    "model = MatrixModel(num_classes = 10,matrix_size = 64).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10)\n",
    "batch_size = 500\n",
    "\n",
    "dataloader = create_batched_data(training_data, training_labels, batch_size=batch_size)\n",
    "print(f\"Training with {len(dataloader)} batches of size {batch_size}\")\n",
    "train_model(model, dataloader, optimizer, scheduler,num_epochs = 80)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  trained_hamiltonians = model().cpu()\n",
    "  trained_eigenvalues = model.eigenvalues.cpu()\n",
    "  trained_eigenvectors = model.get_complex_eigenvectors().cpu()\n",
    "\n",
    "  print(\"\\nTrained Components:\")\n",
    "  for class_idx in range(10):\n",
    "      print(f\"\\nClass {class_idx}:\")\n",
    "      print(f\"Eigenvalues shape: {trained_eigenvalues[class_idx].shape}\")\n",
    "      print(f\"Eigenvectors shape: {trained_eigenvectors[class_idx].shape}\")\n",
    "      print(f\"Hamiltonian shape: {trained_hamiltonians[class_idx].shape}\")\n",
    "      H = trained_hamiltonians[class_idx]\n",
    "      hermitian_error = torch.max(torch.abs(H - H.conj().T))\n",
    "      print(f\"Hermiticity error: {hermitian_error:.2e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxrfIzK4e-cE"
   },
   "source": [
    "Performing the inference\n",
    "\n",
    "visualize_eigenvalues() is used to visualize the trained eigenvalues of the model per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yD-aQOiLe9g_"
   },
   "outputs": [],
   "source": [
    "print(\"Performing inference..\")\n",
    "train_data = torch.as_tensor(normalized_Hermitian_Digit_matrices,dtype = torch.complex128)\n",
    "test_data = torch.as_tensor(normalized_hermitian_matrices_test_input,\n",
    "                               dtype=torch.complex128)\n",
    "\n",
    "predicted_labels , train_acc = inference(model,train_data,torch.tensor(training_labels,dtype = torch.long))\n",
    "print(f\"Train Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "predicted_labels, accuracy = inference(model, test_data, torch.tensor(y_test,dtype = torch.long))\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "def visualize_eigenvalues(model):\n",
    "    with torch.no_grad():\n",
    "        plt.figure(figsize=(15,6))\n",
    "        for c in range(10):\n",
    "            eig = model.eigenvalues[c].cpu().numpy()\n",
    "            plt.subplot(2,5,c+1)\n",
    "            plt.plot(np.sort(eig), 'o--')\n",
    "            plt.title(f'Class {c}')\n",
    "            plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "visualize_eigenvalues(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMdQRM85fQ-b"
   },
   "source": [
    "**Method 2:-**\n",
    "\n",
    "This methid is improvement on the first method.Instead of training on single prototype Hamiltonian per class, the model learns class specific mean eigenvectors and eigenvalues for all 10 digits.During the classification phase, for each test image, the model iteratively optimizes new eigenvalues to best reconstruct the test Hamiltonian using pretrained eigenvectors of each class.The test image is then assigned the label of class that yields lowest reconstruction error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "li0uVSQufwFi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class_sizes = [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]  # Must sum to 60000\n",
    "\n",
    "class ComplexMatrixModel(nn.Module):\n",
    "    def __init__(self, num_classes=10, matrix_size=64):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matrix_size = matrix_size\n",
    "        self.class_sizes = class_sizes\n",
    "        self.eigenvectors = nn.Parameter(\n",
    "            torch.randn(num_classes, matrix_size, matrix_size, dtype=torch.complex128, device=device)\n",
    "        )\n",
    "        self.eigenvalues = nn.Parameter(\n",
    "            torch.randn(num_classes, matrix_size, dtype=torch.float64, device=device)\n",
    "        )\n",
    "        self.class_weights = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(size, matrix_size, dtype=torch.float64, device=device))\n",
    "            for size in self.class_sizes\n",
    "        ])\n",
    "\n",
    "    def make_unitary1(self, matrix):\n",
    "        U, _, Vh = torch.linalg.svd(matrix)\n",
    "        return U @ Vh\n",
    "\n",
    "    def make_unitary2(self,matrix):\n",
    "      Q,R = torch.linalg.qr(matrix)\n",
    "      return Q\n",
    "\n",
    "    def make_unitary3(self,matrix):\n",
    "      matrix = (matrix + matrix.conj().T) / 2\n",
    "      s = torch.matrix_exp((-1j)*matrix)\n",
    "      return s\n",
    "\n",
    "    #CHange 1,2,3\n",
    "    def forward(self, class_idx=None, batch_indices=None):\n",
    "        if class_idx is not None:\n",
    "            return self._generate_class_hamiltonians1(class_idx, batch_indices)\n",
    "        else:\n",
    "            all_hams = []\n",
    "            for idx in range(self.num_classes):\n",
    "                hams = self._generate_class_hamiltonians1(idx)\n",
    "                all_hams.append(hams)\n",
    "            return torch.cat(all_hams, dim=0)\n",
    "\n",
    "    def _generate_class_hamiltonians1(self, class_idx, batch_indices=None):\n",
    "        U = self.make_unitary1(self.eigenvectors[class_idx])\n",
    "        eigvals = self.eigenvalues[class_idx]\n",
    "        if batch_indices is not None:\n",
    "            weights = self.class_weights[class_idx][batch_indices]\n",
    "        else:\n",
    "            weights = self.class_weights[class_idx]\n",
    "\n",
    "        scaled_eig = weights * eigvals.unsqueeze(0)\n",
    "        diag_mats = torch.diag_embed(scaled_eig).to(torch.complex128)\n",
    "        U = U.to(torch.complex128)\n",
    "        U_expanded = U.unsqueeze(0).expand(diag_mats.shape[0], -1, -1)\n",
    "        U_conj_T = U_expanded.conj().transpose(-1, -2)\n",
    "        H = U_expanded @ diag_mats @ U_conj_T\n",
    "        H_hermitian = (H + H.conj().transpose(-1, -2)) / 2\n",
    "        return H_hermitian\n",
    "\n",
    "    def _generate_class_hamiltonians2(self, class_idx, batch_indices=None):\n",
    "        U = self.make_unitary2(self.eigenvectors[class_idx])\n",
    "        eigvals = self.eigenvalues[class_idx]\n",
    "        if batch_indices is not None:\n",
    "            weights = self.class_weights[class_idx][batch_indices]\n",
    "        else:\n",
    "            weights = self.class_weights[class_idx]\n",
    "\n",
    "        scaled_eig = weights * eigvals.unsqueeze(0)\n",
    "        diag_mats = torch.diag_embed(scaled_eig).to(torch.complex128)\n",
    "        U = U.to(torch.complex128)\n",
    "        U_expanded = U.unsqueeze(0).expand(diag_mats.shape[0], -1, -1)\n",
    "        U_conj_T = U_expanded.conj().transpose(-1, -2)\n",
    "        H = U_expanded @ diag_mats @ U_conj_T\n",
    "        H_hermitian = (H + H.conj().transpose(-1, -2)) / 2\n",
    "        return H_hermitian\n",
    "\n",
    "    def _generate_class_hamiltonians3(self, class_idx, batch_indices=None):\n",
    "        U = self.make_unitary3(self.eigenvectors[class_idx])\n",
    "        eigvals = self.eigenvalues[class_idx]\n",
    "        if batch_indices is not None:\n",
    "            weights = self.class_weights[class_idx][batch_indices]\n",
    "        else:\n",
    "            weights = self.class_weights[class_idx]\n",
    "\n",
    "        scaled_eig = weights * eigvals.unsqueeze(0)\n",
    "        diag_mats = torch.diag_embed(scaled_eig).to(torch.complex128)\n",
    "        U = U.to(torch.complex128)\n",
    "        U_expanded = U.unsqueeze(0).expand(diag_mats.shape[0], -1, -1)\n",
    "        U_conj_T = U_expanded.conj().transpose(-1, -2)\n",
    "        H = U_expanded @ diag_mats @ U_conj_T\n",
    "        H_hermitian = (H + H.conj().transpose(-1, -2)) / 2\n",
    "        return H_hermitian\n",
    "\n",
    "\n",
    "    def reconstruct_hamiltonian1(self, class_idx, eigenvalues):\n",
    "        U = self.make_unitary1(self.eigenvectors[class_idx]).to(torch.complex128)\n",
    "        d = torch.diag(eigenvalues.to(torch.complex128))\n",
    "        H = U @ d @ U.conj().T\n",
    "        H_hermitian = (H + H.conj().T)/2\n",
    "        return H_hermitian\n",
    "\n",
    "    def reconstruct_hamiltonian2(self, class_idx, eigenvalues):\n",
    "        U = self.make_unitary2(self.eigenvectors[class_idx]).to(torch.complex128)\n",
    "        d = torch.diag(eigenvalues.to(torch.complex128))\n",
    "        H = U @ d @ U.conj().T\n",
    "        H_hermitian = (H + H.conj().T)/2\n",
    "        return H_hermitian\n",
    "\n",
    "    def reconstruct_hamiltonian3(self, class_idx, eigenvalues):\n",
    "        U = self.make_unitary3(self.eigenvectors[class_idx]).to(torch.complex128)\n",
    "        d = torch.diag(eigenvalues.to(torch.complex128))\n",
    "        H = U @ d @ U.conj().T\n",
    "        H_hermitian = (H + H.conj().T)/2\n",
    "        return H_hermitian\n",
    "\n",
    "def frobenius_loss(output, target):\n",
    "    return torch.mean(torch.linalg.norm(output - target, dim=(-2, -1)))\n",
    "\n",
    "def train_model_batched(model, train_data, train_labels, epochs=10, batch_size=256):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10)\n",
    "\n",
    "    train_data = torch.as_tensor(train_data, dtype=torch.complex128)\n",
    "    train_labels = torch.as_tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "    train_dataset = TensorDataset(train_data, train_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            if batch_count % 100 == 0 :\n",
    "                print(f\"Batch:{batch_count}\")\n",
    "            optimizer.zero_grad()\n",
    "            batch_outputs, batch_targets = [], []\n",
    "            for class_idx in range(10):\n",
    "                mask = (batch_labels == class_idx)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                sample_idx = torch.where(mask)[0]\n",
    "                class_sample_idxs = batch_data.new_tensor(\n",
    "                    [idx.item() for idx in sample_idx], dtype=torch.long, device=device\n",
    "                )\n",
    "                class_local_indices = mask.nonzero().flatten()\n",
    "                class_out = model(class_idx=class_idx, batch_indices=class_local_indices)\n",
    "                batch_outputs.append(class_out)\n",
    "                batch_targets.append(batch_data[mask].to(torch.complex128))\n",
    "\n",
    "            if batch_outputs:\n",
    "                outputs = torch.cat(batch_outputs, dim=0)\n",
    "                targets = torch.cat(batch_targets, dim=0)\n",
    "                loss = frobenius_loss(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "        avg_loss = total_loss / max(batch_count, 1)\n",
    "        scheduler.step(avg_loss)\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Loss: {avg_loss:.5e}\")\n",
    "\n",
    "#Here change 1,2,3\n",
    "import random\n",
    "def classify_train(model, test_hamiltonians, test_labels,optimize_epochs=20, lr=0.1):\n",
    "    # class_counts = [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n",
    "    class_counts = [5923,12665,18623,24754,30596,36017,41935,48200,54051,60000]\n",
    "\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        fixed_eigenvecs = []\n",
    "        for c in range(10):\n",
    "            fixed_eigenvecs.append(model.make_unitary1(model.eigenvectors[c]).to(torch.complex128))\n",
    "    no = 100\n",
    "    for idx, test_H in enumerate(test_hamiltonians):\n",
    "            min_error = float('inf')\n",
    "            best_class = -1\n",
    "            test_H = test_H.to(device).to(torch.complex128)\n",
    "            for class_idx in range(10):\n",
    "                # Optimize eigenvalues for this test_H and class\n",
    "                eigvals = model.eigenvalues[class_idx].detach().clone().to(device).requires_grad_(True)\n",
    "                optimizer = optim.Adam([eigvals], lr=lr)\n",
    "                for _ in range(optimize_epochs):\n",
    "                    optimizer.zero_grad()\n",
    "                    U = fixed_eigenvecs[class_idx]\n",
    "                    dmat = torch.diag(eigvals.to(torch.complex128))\n",
    "                    H = U @ dmat @ U.conj().T\n",
    "                    H_herm = (H + H.conj().T)/2\n",
    "                    loss = torch.linalg.norm(H_herm - test_H)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                err = loss.item()\n",
    "                if err < min_error:\n",
    "                    min_error = err\n",
    "                    best_class = class_idx\n",
    "            results.append(best_class)\n",
    "            if (idx+1)%50 == 0 or idx < 10:\n",
    "                print(f\"Test {idx+1}/{len(test_hamiltonians)} done, best class: {best_class}, loss: {min_error:.3e}\")\n",
    "    return np.array(results)\n",
    "\n",
    "#Change 1,2,3\n",
    "def classify_test(model, test_hamiltonians, test_labels,optimize_epochs=20, lr=0.1):\n",
    "    # class_counts = [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n",
    "\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        fixed_eigenvecs = []\n",
    "        for c in range(10):\n",
    "            fixed_eigenvecs.append(model.make_unitary1(model.eigenvectors[c]).to(torch.complex128))\n",
    "    for idx, test_H in enumerate(test_hamiltonians):\n",
    "            min_error = float('inf')\n",
    "            best_class = -1\n",
    "            test_H = test_H.to(device).to(torch.complex128)\n",
    "            for class_idx in range(10):\n",
    "                # Optimize eigenvalues for this test_H and class\n",
    "                eigvals = model.eigenvalues[class_idx].detach().clone().to(device).requires_grad_(True)\n",
    "                optimizer = optim.Adam([eigvals], lr=lr)\n",
    "                for _ in range(optimize_epochs):\n",
    "                    optimizer.zero_grad()\n",
    "                    U = fixed_eigenvecs[class_idx]\n",
    "                    dmat = torch.diag(eigvals.to(torch.complex128))\n",
    "                    H = U @ dmat @ U.conj().T\n",
    "                    H_herm = (H + H.conj().T)/2\n",
    "                    loss = torch.linalg.norm(H_herm - test_H)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                err = loss.item()\n",
    "                if err < min_error:\n",
    "                    min_error = err\n",
    "                    best_class = class_idx\n",
    "            results.append(best_class)\n",
    "            if (idx+1)%50 == 0 or idx < 10:\n",
    "                print(f\"Test {idx+1}/{len(test_hamiltonians)} done, best class: {best_class}, loss: {min_error:.3e}\")\n",
    "    return np.array(results)\n",
    "\n",
    "\n",
    "def evaluate(pred_labels, true_labels):\n",
    "    acc = (pred_labels == true_labels.cpu().numpy()).mean()\n",
    "    print(f\"\\nTest Accuracy: {acc*100:.2f}%\")\n",
    "    for c in range(10):\n",
    "        mask = (true_labels.cpu().numpy() == c)\n",
    "        acc_c = (pred_labels[mask] == c).mean() if mask.sum() > 0 else np.nan\n",
    "        print(f\"Class {c}: {acc_c*100:.2f}% ({mask.sum()} samples)\")\n",
    "    return acc\n",
    "\n",
    "def visualize_eigenvalues(model):\n",
    "    with torch.no_grad():\n",
    "        plt.figure(figsize=(15,6))\n",
    "        for c in range(10):\n",
    "            eig = model.eigenvalues[c].cpu().numpy()\n",
    "            plt.subplot(2,5,c+1)\n",
    "            plt.plot(np.sort(eig), 'o--')\n",
    "            plt.title(f'Class {c}')\n",
    "            plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWTAnZHEf_WX"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bv_NHdLbgGQ3"
   },
   "source": [
    "We use batch size of 600 ans number of epochs = 80 for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCTAtpMxf_8m"
   },
   "outputs": [],
   "source": [
    "def create_labels_from_class_counts(class_counts):\n",
    "    labels = []\n",
    "    for class_idx, count in enumerate(class_counts):\n",
    "        labels.extend([class_idx] * count)\n",
    "    return labels\n",
    "\n",
    "def create_batched_data(data, labels, batch_size=64):\n",
    "    if not isinstance(data, torch.Tensor):\n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            data = torch.stack(data)\n",
    "        else:\n",
    "            data = torch.tensor(data)\n",
    "    if not isinstance(labels, torch.Tensor):\n",
    "        labels = torch.tensor(labels)\n",
    "    if not data.dtype == torch.complex128:\n",
    "        data = data.to(torch.complex128)\n",
    "    dataset = TensorDataset(data, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "class_counts = [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n",
    "train_hamiltonians = normalized_Hermitian_Digit_matrices\n",
    "train_labels = create_labels_from_class_counts(class_counts)\n",
    "test_hamiltonians = normalized_hermitian_matrices_test_input\n",
    "test_labels = y_test\n",
    "\n",
    "model = ComplexMatrixModel(num_classes=10, matrix_size=64).to(device)\n",
    "print(\"Model Instantiated.\")\n",
    "\n",
    "print(\"----- TRAINING PHASE -----\")\n",
    "train_model_batched(model, train_hamiltonians, np.array(train_labels), epochs=80, batch_size=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ga_SUeohgL4a"
   },
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUld8okFgMns"
   },
   "outputs": [],
   "source": [
    "print(\"----- CLASSIFICATION ON TEST -----\")\n",
    "print(\"Train prediction\")\n",
    "pred_labels_train = classify_train(model,train_hamiltonians,torch.tensor(train_labels,dtype = torch.long),optimize_epochs=20,lr = 0.1)\n",
    "print(\"test prediction\")\n",
    "pred_labels_test = classify_test(model, test_hamiltonians, torch.tensor(test_labels[:1000],dtype = torch.long), optimize_epochs=20, lr=0.1)\n",
    "\n",
    "print(\"----- ACCURACY -----\")\n",
    "train_accuracy = evaluate(pred_labels_train, torch.tensor(training_labels,dtype = torch.long))\n",
    "test_accuracy = evaluate(pred_labels_test, torch.tensor(y_test,dtype = torch.long))\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPUH/jorLYFb6ZHtcTYwdZ0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
