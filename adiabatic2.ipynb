{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Preprocessing of the MNIST Dataset to produce the train and test normalized Hamiitonians...\n# We can construct the hamiltonians from the four methods described in the paper...\nimport numpy as np\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nfrom sklearn.datasets import fetch_openml\nimport scipy\n\n# ----------------------------\n# Load MNIST from OpenML\n# ----------------------------\n# mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n\n# mnist_images = mnist.data.astype(np.float64)   # (70000, 784)\n# mnist_labels = mnist.target.astype(int)        # (70000,)\n\n# x_train = mnist_images[:60000]\n# y_train = mnist_labels[:60000]\n# x_test  = mnist_images[60000:60201]\n# y_test  = mnist_labels[60000:60201]\n\n#OR\n\nfrom tensorflow.keras.datasets import mnist\n\n# Load MNIST using Keras\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# # Reshape and convert to float64 for consistency\nx_train = x_train.reshape(-1, 784).astype(np.float64)\nx_test = x_test.reshape(-1, 784).astype(np.float64)\n\nprint(\"Train:\", x_train.shape, y_train.shape)\nprint(\"Test:\", x_test.shape, y_test.shape)\n\n# ----------------------------\n# Helper functions\n# ----------------------------\ndef separate_digits(images, labels):\n    \"\"\"Group images by digit label.\"\"\"\n    digit_image = {d: [] for d in range(10)}\n    for img, lbl in zip(images, labels):\n        digit_image[lbl].append(img)\n    return digit_image\n\ndef resize_images_batch(images, new_size=(8, 8), batch_size=500):\n    \"\"\"Resize a batch of flattened 28x28 images to new_size.\"\"\"\n    n = len(images)\n    resized = []\n    for i in range(0, n, batch_size):\n        batch = images[i:i+batch_size]\n        resized_batch = [resize(img.reshape(28,28), new_size).flatten() for img in batch]\n        resized.extend(resized_batch)\n    return np.array(resized)\n\ndef normalize_batch(images):\n    \"\"\"Normalize each image vector.\"\"\"\n    norms = np.linalg.norm(images, axis=1, keepdims=True)\n    return images / norms\n\n#Creating Hamiltonian using outer product method\ndef density_matrix_batch(images):\n    \"\"\"Convert vectors to density matrices.\"\"\"\n    return np.matmul(images[:,:,np.newaxis], images[:,np.newaxis,:])\n\n#Creating the Hamiltonian using H = A + A.T/2 method\ndef hamiltonian_symmetric_batch(images):\n  N,D = images.shape\n  H_list = []\n  for i in range(N):\n    a = images[i]\n    A = np.outer(a,np.ones(D))\n    H = (A + A.conj().T) / 2\n    H_list.append(H)\n\n  return np.array(H_list)\n\n#Creating the Hamiltonian using H = A @ A.T method\ndef hamiltonian_product_batch(images):\n  N , D = images.shape\n  H_list = []\n  for i in range(N):\n    a = images[i]\n    A = np.outer(a,np.ones(D))\n    H = A @ A.T\n    H_list.append(H)\n  return np.array(H_list)\n\nimport scipy.linalg\n#Creating thr Hamiltonian using H = -i * log(V) method\ndef hamiltonian_using_log(images):\n    def make_unitary1(matrix):\n        U, _, Vh = torch.linalg.svd(matrix, full_matrices=False)\n        return U @ Vh\n    N , D = images.shape\n    hamiltonians = np.zeros((N,D,D),dtype = torch.complex128)\n    for i in range(N):\n        image = images[i]\n        mat = np.diag(image)\n        mat_torch = torch.tensor(mat,dtype = torch.complex128)\n        U_torch = make_unitary1(mat_torch)\n        U_np = U_torch.detach().cpu().numpy()\n        H = -1j * scipy.linalg.logm(U_np)\n        H = (H + H.conj().T) / 2\n        hamiltonians[i] = H\n\n    return hamiltonians\n        \n\n# ----------------------------\n# Process training data\n# ----------------------------\ndigit_images_dict = separate_digits(x_train, y_train)\nresized_digit_images = {}\nnormalized_digit_images = {}\ndensity_matrices = {}\n\nfor digit, imgs in digit_images_dict.items():\n    imgs = np.array(imgs)\n    imgs_resized = resize_images_batch(imgs, new_size=(8,8), batch_size=500)\n    imgs_normalized = normalize_batch(imgs_resized)\n    print(f\"normalized_images shape:- {imgs_normalized.shape}\")\n    density1 = density_matrix_batch(imgs_normalized)\n    print(f\"shape 1:- {density1.shape}\")\n    #OR\n    density2 = hamiltonian_symmetric_batch(imgs_normalized)\n    print(f\"shape 2:- {density2.shape}\")\n    #OR\n    density3 = hamiltonian_product_batch(imgs_normalized)\n    print(f\"shape 3:- {density3.shape}\")\n    density4 = hamiltonian_using_log(imgs_normalized)\n    print(f\"shape 4:- {density4.shape}\")\n    density1 /= np.linalg.norm(density1, axis=(1,2), keepdims=True)\n    density2 /= np.linalg.norm(density2, axis=(1,2), keepdims=True)\n    density3 /= np.linalg.norm(density3, axis=(1,2), keepdims=True)\n    density4 /= np.linalg.norm(density4, axis=(1,2), keepdims=True)\n    resized_digit_images[digit] = imgs_resized\n    normalized_digit_images[digit] = imgs_normalized\n    density_matrices[digit] = density1\n    #density_matrices[digit] = density2\n    #OR density_matrices[digit] = density3\n    #density_matrices[digit] = density4\n\ntrain_density_matrices = np.concatenate([density_matrices[d] for d in range(10)], axis=0)\ntrain_density_matrices_tensor = torch.tensor(train_density_matrices, dtype=torch.cfloat)\n\n# ----------------------------\n# Process test data\n# ----------------------------\ntest_images_resized = np.array([resize(img.reshape(28,28), (8,8)).flatten() for img in x_test])\ntest_normed = normalize_batch(test_images_resized)\ntest_density = density_matrix_batch(test_normed)\ntest_density /= np.linalg.norm(test_density, axis=(1,2), keepdims=True)\ntest_density_tensor = torch.tensor(test_density, dtype=torch.cfloat)\n\n# ----------------------------\n# Visualization example\n# ---------------------------\n\nfor digit in range(10):\n    images_to_plot = resized_digit_images[digit][:10]\n    plt.figure(figsize=(10,2))\n    for i in range(10):\n        plt.subplot(1, 10, i+1)\n        plt.imshow(images_to_plot[i].reshape(8,8), cmap='magma')\n        plt.title(f\"{digit}\")\n        plt.axis('off')\n    plt.show()\n\nnormalized_Hermitian_Digit_matrices = train_density_matrices_tensor\n# normalized_Hermitian_Digit_matrices = torch.tensor(train_subset,dtype = torch.cfloat)\nnormalized_hermitian_matrices_test_input = test_density_tensor\n# normalized_hermitian_matrices_test_input = torch.tensor(test_subset,dtype = torch.cfloat)\n\nprint(f\"normalized_Hermitian_Digit_matrices shape:- {normalized_Hermitian_Digit_matrices.shape}\")\nprint(f\"normalized_hermitian_matrices_test_input shape:- {normalized_hermitian_matrices_test_input.shape}\")\n# normalized_Hermitian_Digit_matrices_small = torch.tensor(train_subset_small,dtype = torch.cfloat)\n# normalized_hermitian_matrices_test_input_small = torch.tensor(test_subset_small,dtype = torch.cfloat)\n\n\nlabels = []\nfor i in range(10):\n    labels.append(i)\n\nprint(labels)\n\nD = [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n# D = [100] * 10\nlabels_zero = [labels[0]]*D[0]\nlabels_one  = [labels[1]]*D[1]\nlabels_two  = [labels[2]]*D[2]\nlabels_three  = [labels[3]]*D[3]\nlabels_four  = [labels[4]]*D[4]\nlabels_five  = [labels[5]]*D[5]\nlabels_six  = [labels[6]]*D[6]\nlabels_seven  = [labels[7]]*D[7]\nlabels_eigth  = [labels[8]]*D[8]\nlabels_nineth  = [labels[9]]*D[9]\nlabels_zero = np.array(labels_zero,dtype = int)\nlabels_one = np.array(labels_one,dtype = int)\nlabels_two = np.array(labels_two,dtype = int)\nlabels_three = np.array(labels_three,dtype = int)\nlabels_four = np.array(labels_four,dtype = int)\nlabels_five = np.array(labels_five,dtype = int)\nlabels_six = np.array(labels_six,dtype = int)\nlabels_seven = np.array(labels_seven,dtype = int)\nlabels_eigth = np.array(labels_eigth,dtype = int)\nlabels_nineth = np.array(labels_nineth,dtype = int)\n\nlabels_new_train = np.concatenate((labels_zero,labels_one))\nlabels_new_train = np.concatenate((labels_new_train,labels_two))\nlabels_new_train = np.concatenate((labels_new_train,labels_three))\nlabels_new_train = np.concatenate((labels_new_train,labels_four))\nlabels_new_train = np.concatenate((labels_new_train,labels_five))\nlabels_new_train = np.concatenate((labels_new_train,labels_six))\nlabels_new_train = np.concatenate((labels_new_train,labels_seven))\nlabels_new_train = np.concatenate((labels_new_train,labels_eigth))\nlabels_new_train = np.concatenate((labels_new_train,labels_nineth))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T08:42:38.474401Z","iopub.execute_input":"2025-08-11T08:42:38.474758Z","iopub.status.idle":"2025-08-11T08:43:09.677268Z","shell.execute_reply.started":"2025-08-11T08:42:38.474736Z","shell.execute_reply":"2025-08-11T08:43:09.676489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\n\nclass MatrixModel(nn.Module):\n  def __init__(self,num_classes = 10, matrix_size = 64):\n    super().__init__()\n    self.num_classes = num_classes\n    self.matrix_size = matrix_size\n    self.eigenvalues = nn.Parameter(torch.randn(num_classes,matrix_size,dtype = torch.float64))\n    self.eigenvectors_real = nn.Parameter(torch.randn(num_classes,matrix_size,matrix_size,dtype = torch.float64))\n    self.eigenvectors_imag = nn.Parameter(torch.randn(num_classes,matrix_size,matrix_size,dtype = torch.float64))\n\n  def get_complex_eigenvectors(self):\n    return torch.complex(self.eigenvectors_real, self.eigenvectors_imag)\n\n  def find_unitary_transformation(input_density_matrix, output_density_matrix):\n    X = np.dot(output_density_matrix, np.linalg.pinv(input_density_matrix))\n    U, S, V_dagger = np.linalg.svd(X, full_matrices=False)\n    phase_matrix = np.diag(np.exp(1j * np.angle(S)))\n    unitary_matrix = U @ (phase_matrix @ V_dagger)\n    unitary_matrix /= np.linalg.det(unitary_matrix)**(1/2)\n    return unitary_matrix\n\n  def is_unitary(matrix):\n    # Check if the matrix is unitary\n    identity = np.eye(matrix.shape[0])\n    return np.allclose(matrix @ matrix.conj().T, identity) and np.allclose(matrix.conj().T @ matrix, identity)\n\n\n  def make_unitary1(self,matrix):\n    U,_,Vh = torch.linalg.svd(matrix,full_matrices = False)\n    return U @ Vh\n\n  def make_unitary2(self,matrix):\n    Q,R = torch.linalg.qr(matrix)\n    return Q\n\n  def make_unitary3(self, matrix, input_density_matrix):\n    # Lower triangular density matrix\n    lower_triangular = np.tril(matrix)\n    lower_triangular_conj = np.conj(lower_triangular).T\n    Density_Matrix_Classical_Lower = lower_triangular + lower_triangular_conj\n\n    # Upper triangular density matrix\n    upper_triangular = np.triu(matrix)\n    upper_triangular_conj = np.conj(upper_triangular).T\n    Density_Matrix_Classical_Upper = upper_triangular + upper_triangular_conj\n\n    # Halve diagonal elements\n    np.fill_diagonal(Density_Matrix_Classical_Lower,\n                     Density_Matrix_Classical_Lower.diagonal() / 2)\n    np.fill_diagonal(Density_Matrix_Classical_Upper,\n                     Density_Matrix_Classical_Upper.diagonal() / 2)\n\n    # Normalize by trace\n    Density_Matrix_Classical_Lower_Normalized = (\n        Density_Matrix_Classical_Lower / np.trace(Density_Matrix_Classical_Lower)\n    )\n    Density_Matrix_Classical_Upper_Normalized = (\n        Density_Matrix_Classical_Upper / np.trace(Density_Matrix_Classical_Upper)\n    )\n\n    # Find unitary transformations\n    unitary_transformation_lower = find_unitary_transformation(\n        input_density_matrix, Density_Matrix_Classical_Lower_Normalized\n    )\n    unitary_transformation_upper = find_unitary_transformation(\n        input_density_matrix, Density_Matrix_Classical_Upper_Normalized\n    )\n\n    # Choose one — here we return the lower version\n    return unitary_transformation_lower\n  \n\n    \n      \n\n  \n  def get_hamiltonians_orig1(self):\n        eigenvectors_complex = self.get_complex_eigenvectors()\n        unitary_vecs = torch.stack([self.make_unitary1(mat) for mat in eigenvectors_complex])\n        diag_matrices = torch.diag_embed(self.eigenvalues.to(torch.complex128))\n        hamiltonians = unitary_vecs @ diag_matrices @ unitary_vecs.conj().transpose(-1, -2)\n        hamiltonians = (hamiltonians + hamiltonians.conj().transpose(-1, -2)) / 2\n        return hamiltonians\n  \n  def get_hamiltonians_orig2(self):\n        eigenvectors_complex = self.get_complex_eigenvectors()\n        unitary_vecs = torch.stack([self.make_unitary2(mat) for mat in eigenvectors_complex])\n        diag_matrices = torch.diag_embed(self.eigenvalues.to(torch.complex128))\n        hamiltonians = unitary_vecs @ diag_matrices @ unitary_vecs.conj().transpose(-1, -2)\n        hamiltonians = (hamiltonians + hamiltonians.conj().transpose(-1, -2)) / 2\n        return hamiltonians\n  \n  def get_hamiltonians_orig3(self):\n        eigenvectors_complex = self.get_complex_eigenvectors()\n        unitary_vecs = torch.stack([self.make_unitary3(mat) for mat in eigenvectors_complex])\n        diag_matrices = torch.diag_embed(self.eigenvalues.to(torch.complex128))\n        hamiltonians = unitary_vecs @ diag_matrices @ unitary_vecs.conj().transpose(-1, -2)\n        hamiltonians = (hamiltonians + hamiltonians.conj().transpose(-1, -2)) / 2\n        return hamiltonians\n  \n\n\n\n  def forward(self):\n    return self.get_hamiltonians_orig2() #OR get_hamiltonians_orig2,get_hamiltonians_orig3\n\ndef combined_loss_batched(output, target_batch, labels_batch):\n  batch_size = target_batch.size(0)\n  class_hamiltonians = output[labels_batch]\n  # print(class_hamiltonians.shape)\n  # print(target_batch.shape)\n  losses = torch.linalg.norm(class_hamiltonians - target_batch,dim = (1,2))\n  return torch.mean(losses)\n\ndef combined_loss_batched2(output, target_batch, labels_batch):\n    batch_size = target_batch.size(0)\n\n    # Differentiable class selection\n    class_hamiltonians = torch.gather(\n        output, 1, labels_batch.view(-1, 1, 1, 1).expand(-1, 1, 64, 64)\n    ).squeeze(1)\n\n    # Convert target to Hamiltonian form\n    target_batch = target_batch.reshape(batch_size, -1)  # (n, 64)\n    target_batch = torch.einsum('bi,bj->bij', target_batch, target_batch.conj())  # (n, 64, 64)\n\n    losses = torch.linalg.norm(class_hamiltonians - target_batch, dim=(1, 2))\n    return torch.mean(losses)\n\n\ndef create_labels_from_class_counts(class_counts):\n    labels = []\n    for class_idx, count in enumerate(class_counts):\n        labels.extend([class_idx] * count)\n    return labels\n\ndef create_batched_data(data, labels, batch_size=64):\n    if not isinstance(data, torch.Tensor):\n        if isinstance(data, list) and len(data) > 0:\n            data = torch.stack(data)\n        else:\n            data = torch.tensor(data)\n    if not isinstance(labels, torch.Tensor):\n        labels = torch.tensor(labels)\n    if not data.dtype == torch.complex128:\n        data = data.to(torch.complex128)\n    dataset = TensorDataset(data, labels)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    return dataloader\n\ndef train_model(model, dataloader,optimizer, scheduler, threshold = 0.0000001, num_epochs=100):\n    model.train()\n    all_losses = []\n    wait = 0\n    patience = 5\n    to_stop = 0\n    epoch = 0\n    for epoch in range(num_epochs):\n        print(f\"Epoch:- {epoch}\")\n        total_loss = 0.0\n        num_batches = 0\n\n        for batch_data, batch_labels in dataloader:\n            if num_batches % 1000 == 0:\n              print(num_batches)\n            batch_data = batch_data.to(device)\n            batch_labels = batch_labels.to(device)\n            optimizer.zero_grad()\n            outputs = model()\n            loss = combined_loss_batched(outputs, batch_data, batch_labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            num_batches += 1\n\n        avg_loss = total_loss / num_batches\n        scheduler.step(avg_loss)\n        diff = 0\n        if epoch > 2:\n          diff = all_losses[-1] - avg_loss\n        all_losses.append(avg_loss)\n        if epoch > 2 and avg_loss - all_losses[-1] < threshold:\n          print(\"less than threshold\")\n          if wait < patience:\n            wait = wait + 1\n          else:\n            to_stop = 1\n        if epoch % 1 == 0:\n            print(f'Epoch [{epoch}/{num_epochs}], Average Loss: {avg_loss:.4e}, Difference = {diff:.10e}')\n        epoch = epoch + 1\n\n    print(\"Training completed!\")\n\ndef inference(model, test_data, test_labels=None):\n    model.eval()\n\n    with torch.no_grad():\n        hamiltonians = model().cpu() # 10,64,64\n        # print(hamiltonians.shape)\n        predicted_labels = []\n\n        for test_sample in test_data:\n            if isinstance(test_sample, torch.Tensor):\n                test_sample = test_sample.cpu()\n            frobenius_norms = []\n            for class_idx in range(10):\n                # print(test_sample.shape)\n                # print(hamiltonians[class_idx].shape)\n                norm = torch.linalg.norm(test_sample - hamiltonians[class_idx], ord='fro')\n                frobenius_norms.append(norm.item())\n            predicted_labels.append(np.argmin(frobenius_norms))\n        if test_labels is not None:\n            accuracy = np.sum(np.array(predicted_labels) == np.array(test_labels))\n            accuracy_percent = (accuracy / len(test_labels)) * 100\n            return predicted_labels, accuracy_percent\n\n        return predicted_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T09:07:49.654637Z","iopub.execute_input":"2025-08-11T09:07:49.654961Z","iopub.status.idle":"2025-08-11T09:07:49.687467Z","shell.execute_reply.started":"2025-08-11T09:07:49.654938Z","shell.execute_reply":"2025-08-11T09:07:49.686381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import scipy\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Creating the training data and labels\")\nclass_counts = [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n# class_counts = [100] * 10\ntraining_labels = create_labels_from_class_counts(class_counts)\ntraining_data = torch.as_tensor(normalized_Hermitian_Digit_matrices,dtype = torch.complex128,device = device)\nprint(f\"training data shape:- {training_data.shape}\")\n# training_data_small = torch.as_tensor(normalized_Hermitian_Digit_matrices_small.view(-1,8,8))\n# print(f\"training data small shape:- {training_data_small.shape}\")\n\n\nprint(\"Initialising the model,optimier,scheduler\")\nmodel = MatrixModel(num_classes = 10,matrix_size = 64).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10)\nbatch_size = 15\n\ndataloader = create_batched_data(training_data, training_labels, batch_size=batch_size)\n# dataloader_small = create_batched_data(training_data_small,training_labels,batch_size = batch_size)\n\nprint(f\"Training with {len(dataloader)} batches of size {batch_size}\")\ntrain_model(model, dataloader, optimizer, scheduler,num_epochs = 10)\n\nmodel.eval()\nwith torch.no_grad():\n  trained_hamiltonians = model().cpu()\n  trained_eigenvalues = model.eigenvalues.cpu()\n  trained_eigenvectors = model.get_complex_eigenvectors().cpu()\n\n  print(\"\\nTrained Components:\")\n  for class_idx in range(10):\n      print(f\"\\nClass {class_idx}:\")\n      print(f\"Eigenvalues shape: {trained_eigenvalues[class_idx].shape}\")\n      print(f\"Eigenvectors shape: {trained_eigenvectors[class_idx].shape}\")\n      print(f\"Hamiltonian shape: {trained_hamiltonians[class_idx].shape}\")\n      H = trained_hamiltonians[class_idx]\n      hermitian_error = torch.max(torch.abs(H - H.conj().T))\n      print(f\"Hermiticity error: {hermitian_error:.2e}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T09:07:53.867899Z","iopub.execute_input":"2025-08-11T09:07:53.868661Z","iopub.status.idle":"2025-08-11T09:18:29.953980Z","shell.execute_reply.started":"2025-08-11T09:07:53.868634Z","shell.execute_reply":"2025-08-11T09:18:29.953091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Performing inference..\")\ntrain_data = torch.as_tensor(normalized_Hermitian_Digit_matrices,dtype = torch.complex128)\n\ntrain_target = [0]*100 + [1]*100 + [2]*100 + [3]*100 + [4]*100 + [5]*100 + [6]*100 + [7]*100 + [8]*100 + [9]*100\n\ntest_data = torch.as_tensor(normalized_hermitian_matrices_test_input,\n                               dtype=torch.complex128)\ntest_target = [0]*20 + [1]*20 + [2]*20 + [3]*20 + [4]*20 + [5]*20 + [6]*20 + [7]*20 + [8]*20 + [9]*20\n\npredicted_labels , train_acc = inference(model,train_data,torch.tensor(training_labels,dtype = torch.long))\nprint(f\"Train Accuracy: {train_acc:.2f}%\")\n\npredicted_labels, accuracy = inference(model, test_data, torch.tensor(y_test,dtype = torch.long))\nprint(f\"Test Accuracy: {accuracy:.2f}%\")\n\ndef visualize_eigenvalues(model):\n    with torch.no_grad():\n        plt.figure(figsize=(15,6))\n        for c in range(10):\n            eig = model.eigenvalues[c].cpu().numpy()\n            plt.subplot(2,5,c+1)\n            plt.plot(np.sort(eig), 'o--')\n            plt.title(f'Class {c}')\n            plt.grid(True)\n        plt.tight_layout()\n        plt.show()\n\nvisualize_eigenvalues(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T09:21:15.816172Z","iopub.execute_input":"2025-08-11T09:21:15.816594Z","iopub.status.idle":"2025-08-11T09:22:29.997308Z","shell.execute_reply.started":"2025-08-11T09:21:15.816568Z","shell.execute_reply":"2025-08-11T09:22:29.996447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}